{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InvenCare SageMaker Forecasting Pipeline\n",
    "\n",
    "This notebook connects SageMaker to your RDS database and updates forecasting data.\n",
    "Designed to run in AWS SageMaker Jupyter Lab environment.\n",
    "\n",
    "## Features:\n",
    "- Connect to RDS database\n",
    "- Generate demand predictions\n",
    "- Update forecasting models\n",
    "- Visualize results\n",
    "- Schedule daily runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only once)\n",
    "!pip install mysql-connector-python pandas numpy scikit-learn matplotlib seaborn plotly\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# AWS imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Database imports\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# ML imports\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Notebook run time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS and SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"üîß SageMaker Role: {role}\")\n",
    "print(f\"üåç AWS Region: {region}\")\n",
    "print(f\"üì¶ SageMaker SDK Version: {sagemaker.__version__}\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "runtime_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Get default S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'invencare-forecasting'\n",
    "\n",
    "print(f\"ü™£ Default S3 Bucket: {bucket}\")\n",
    "print(f\"üìÇ S3 Prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "# Update these values with your RDS details\n",
    "DB_CONFIG = {\n",
    "    'host': 'invencaredb.cihe2wg8etco.us-east-1.rds.amazonaws.com',  # Your RDS endpoint\n",
    "    'user': 'admin',\n",
    "    'password': 'InvenCare123',  # Store securely in AWS Secrets Manager in production\n",
    "    'database': 'invencare',\n",
    "    'port': 3306,\n",
    "    'autocommit': False\n",
    "}\n",
    "\n",
    "def connect_to_database():\n",
    "    \"\"\"Establish connection to RDS database\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        print(\"‚úÖ Database connection successful\")\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_database_connection():\n",
    "    \"\"\"Test database connectivity and show basic info\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Test basic queries\n",
    "        cursor.execute(\"SELECT COUNT(*) as store_count FROM stores WHERE status = 'active'\")\n",
    "        store_count = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) as product_count FROM products WHERE status = 'active'\")\n",
    "        product_count = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"üè™ Active Stores: {store_count}\")\n",
    "        print(f\"üì¶ Active Products: {product_count}\")\n",
    "        \n",
    "        # Check forecasting tables\n",
    "        cursor.execute(\"SHOW TABLES LIKE 'demand_%'\")\n",
    "        forecasting_tables = cursor.fetchall()\n",
    "        print(f\"üìä Forecasting Tables: {len(forecasting_tables)}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Test the connection\n",
    "test_database_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_historical_data():\n",
    "    \"\"\"Load historical sales and inventory data\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if not conn:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load products data\n",
    "    products_query = \"\"\"\n",
    "    SELECT p.id, p.name, p.category, p.price, p.quantity, p.minimum_stock,\n",
    "           p.store_id, s.name as store_name\n",
    "    FROM products p\n",
    "    JOIN stores s ON p.store_id = s.id\n",
    "    WHERE p.status = 'active' AND s.status = 'active'\n",
    "    \"\"\"\n",
    "    \n",
    "    products_df = pd.read_sql(products_query, conn)\n",
    "    \n",
    "    # Load transaction history\n",
    "    transactions_query = \"\"\"\n",
    "    SELECT it.product_id, it.store_id, it.transaction_type, it.quantity,\n",
    "           it.total_amount, it.created_at, p.name as product_name, \n",
    "           p.category, s.name as store_name\n",
    "    FROM inventory_transactions it\n",
    "    JOIN products p ON it.product_id = p.id\n",
    "    JOIN stores s ON it.store_id = s.id\n",
    "    WHERE it.created_at >= DATE_SUB(NOW(), INTERVAL 90 DAY)\n",
    "    ORDER BY it.created_at DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    transactions_df = pd.read_sql(transactions_query, conn)\n",
    "    \n",
    "    # Load existing predictions (if any)\n",
    "    predictions_query = \"\"\"\n",
    "    SELECT dp.*, p.name as product_name, s.name as store_name\n",
    "    FROM demand_predictions dp\n",
    "    JOIN products p ON dp.product_id = p.id\n",
    "    JOIN stores s ON dp.store_id = s.id\n",
    "    WHERE dp.prediction_date >= CURDATE()\n",
    "    ORDER BY dp.prediction_date ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        predictions_df = pd.read_sql(predictions_query, conn)\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è No existing predictions table found\")\n",
    "        predictions_df = pd.DataFrame()\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return products_df, transactions_df, predictions_df\n",
    "\n",
    "# Load data\n",
    "print(\"üìä Loading historical data...\")\n",
    "products_df, transactions_df, predictions_df = load_historical_data()\n",
    "\n",
    "if products_df is not None:\n",
    "    print(f\"‚úÖ Loaded {len(products_df)} products\")\n",
    "    print(f\"‚úÖ Loaded {len(transactions_df)} transactions\")\n",
    "    print(f\"‚úÖ Loaded {len(predictions_df)} existing predictions\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nüìà Data Overview:\")\n",
    "    print(f\"Date range: {transactions_df['created_at'].min()} to {transactions_df['created_at'].max()}\")\n",
    "    print(f\"Categories: {products_df['category'].nunique()}\")\n",
    "    print(f\"Stores: {products_df['store_id'].nunique()}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize current inventory status\n",
    "if products_df is not None and len(products_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Products by category\n",
    "    category_counts = products_df['category'].value_counts()\n",
    "    axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Products by Category')\n",
    "    \n",
    "    # 2. Inventory levels\n",
    "    products_df['stock_status'] = pd.cut(products_df['quantity'], \n",
    "                                       bins=[0, 10, 50, 100, float('inf')], \n",
    "                                       labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    stock_counts = products_df['stock_status'].value_counts()\n",
    "    axes[0, 1].bar(stock_counts.index, stock_counts.values)\n",
    "    axes[0, 1].set_title('Inventory Stock Levels')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Price distribution\n",
    "    axes[1, 0].hist(products_df['price'], bins=20, alpha=0.7)\n",
    "    axes[1, 0].set_title('Price Distribution')\n",
    "    axes[1, 0].set_xlabel('Price ($)')\n",
    "    \n",
    "    # 4. Products per store\n",
    "    store_counts = products_df['store_name'].value_counts()\n",
    "    axes[1, 1].bar(range(len(store_counts)), store_counts.values)\n",
    "    axes[1, 1].set_title('Products per Store')\n",
    "    axes[1, 1].set_xticks(range(len(store_counts)))\n",
    "    axes[1, 1].set_xticklabels(store_counts.index, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Current inventory visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction patterns\n",
    "if transactions_df is not None and len(transactions_df) > 0:\n",
    "    # Convert date column\n",
    "    transactions_df['date'] = pd.to_datetime(transactions_df['created_at']).dt.date\n",
    "    \n",
    "    # Daily sales trends\n",
    "    daily_sales = transactions_df[transactions_df['transaction_type'] == 'sale'].groupby('date').agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "    \n",
    "    # Sales volume over time\n",
    "    axes[0].plot(daily_sales['date'], daily_sales['quantity'], marker='o')\n",
    "    axes[0].set_title('Daily Sales Volume')\n",
    "    axes[0].set_ylabel('Quantity Sold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sales revenue over time\n",
    "    axes[1].plot(daily_sales['date'], daily_sales['total_amount'], marker='o', color='green')\n",
    "    axes[1].set_title('Daily Sales Revenue')\n",
    "    axes[1].set_ylabel('Revenue ($)')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Transaction type breakdown\n",
    "    transaction_summary = transactions_df.groupby('transaction_type').agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìä Transaction Summary (Last 90 days):\")\n",
    "    print(transaction_summary)\n",
    "    \n",
    "    print(\"üìà Transaction pattern analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demand Forecasting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvenCareForecastingPipeline:\n",
    "    def __init__(self, db_config):\n",
    "        self.db_config = db_config\n",
    "        self.connection = None\n",
    "        \n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to database\"\"\"\n",
    "        try:\n",
    "            self.connection = mysql.connector.connect(**self.db_config)\n",
    "            return True\n",
    "        except Error as e:\n",
    "            print(f\"Database connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def disconnect_database(self):\n",
    "        \"\"\"Disconnect from database\"\"\"\n",
    "        if self.connection and self.connection.is_connected():\n",
    "            self.connection.close()\n",
    "    \n",
    "    def prepare_forecasting_data(self, product_id, store_id):\n",
    "        \"\"\"Prepare historical data for forecasting\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT DATE(created_at) as date, \n",
    "               SUM(CASE WHEN transaction_type = 'sale' THEN ABS(quantity) ELSE 0 END) as demand\n",
    "        FROM inventory_transactions \n",
    "        WHERE product_id = %s AND store_id = %s \n",
    "        AND created_at >= DATE_SUB(NOW(), INTERVAL 90 DAY)\n",
    "        GROUP BY DATE(created_at)\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, self.connection, params=(product_id, store_id))\n",
    "        \n",
    "        if len(df) < 7:  # Need at least a week of data\n",
    "            return None\n",
    "            \n",
    "        # Add time features\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['day_of_month'] = df['date'].dt.day\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def simple_demand_forecast(self, historical_data, forecast_days=30):\n",
    "        \"\"\"Simple forecasting using moving averages and trends\"\"\"\n",
    "        if historical_data is None or len(historical_data) < 7:\n",
    "            return None\n",
    "        \n",
    "        # Calculate moving averages\n",
    "        ma_7 = historical_data['demand'].rolling(window=7, min_periods=1).mean().iloc[-1]\n",
    "        ma_14 = historical_data['demand'].rolling(window=14, min_periods=1).mean().iloc[-1]\n",
    "        \n",
    "        # Calculate trend\n",
    "        recent_avg = historical_data['demand'].tail(7).mean()\n",
    "        older_avg = historical_data['demand'].head(7).mean() if len(historical_data) >= 14 else recent_avg\n",
    "        trend = (recent_avg - older_avg) / max(older_avg, 1)\n",
    "        \n",
    "        # Weekly seasonality (simple)\n",
    "        weekly_pattern = historical_data.groupby('day_of_week')['demand'].mean()\n",
    "        weekly_avg = weekly_pattern.mean()\n",
    "        seasonality_factors = (weekly_pattern / max(weekly_avg, 1)).to_dict()\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = []\n",
    "        base_date = historical_data['date'].max()\n",
    "        \n",
    "        for i in range(1, forecast_days + 1):\n",
    "            pred_date = base_date + timedelta(days=i)\n",
    "            day_of_week = pred_date.dayofweek\n",
    "            \n",
    "            # Base prediction using weighted average of moving averages\n",
    "            base_demand = 0.6 * ma_7 + 0.4 * ma_14\n",
    "            \n",
    "            # Apply trend\n",
    "            trend_factor = 1 + (trend * i / 30)  # Gradual trend application\n",
    "            \n",
    "            # Apply seasonality\n",
    "            seasonal_factor = seasonality_factors.get(day_of_week, 1.0)\n",
    "            \n",
    "            # Final prediction\n",
    "            predicted_demand = base_demand * trend_factor * seasonal_factor\n",
    "            \n",
    "            # Add some noise for confidence intervals\n",
    "            std_dev = historical_data['demand'].std()\n",
    "            confidence_lower = max(0, predicted_demand - 1.96 * std_dev)\n",
    "            confidence_upper = predicted_demand + 1.96 * std_dev\n",
    "            \n",
    "            predictions.append({\n",
    "                'prediction_date': pred_date.strftime('%Y-%m-%d'),\n",
    "                'predicted_demand': round(predicted_demand, 2),\n",
    "                'confidence_interval_lower': round(confidence_lower, 2),\n",
    "                'confidence_interval_upper': round(confidence_upper, 2),\n",
    "                'factors': {\n",
    "                    'base_demand': round(base_demand, 2),\n",
    "                    'trend_factor': round(trend_factor, 3),\n",
    "                    'seasonal_factor': round(seasonal_factor, 3)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def update_demand_predictions(self, predictions, product_id, store_id, model_id=1):\n",
    "        \"\"\"Update demand predictions in database\"\"\"\n",
    "        if not predictions:\n",
    "            return 0\n",
    "        \n",
    "        cursor = self.connection.cursor()\n",
    "        \n",
    "        # Check if demand_predictions table exists\n",
    "        cursor.execute(\"SHOW TABLES LIKE 'demand_predictions'\")\n",
    "        if not cursor.fetchone():\n",
    "            print(\"‚ö†Ô∏è demand_predictions table not found, skipping update\")\n",
    "            return 0\n",
    "        \n",
    "        # Insert predictions\n",
    "        query = \"\"\"\n",
    "        INSERT INTO demand_predictions \n",
    "        (product_id, store_id, model_id, prediction_date, predicted_demand,\n",
    "         confidence_interval_lower, confidence_interval_upper, factors)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON DUPLICATE KEY UPDATE\n",
    "        predicted_demand = VALUES(predicted_demand),\n",
    "        confidence_interval_lower = VALUES(confidence_interval_lower),\n",
    "        confidence_interval_upper = VALUES(confidence_interval_upper),\n",
    "        factors = VALUES(factors)\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction_values = []\n",
    "        for pred in predictions:\n",
    "            values = (\n",
    "                product_id,\n",
    "                store_id,\n",
    "                model_id,\n",
    "                pred['prediction_date'],\n",
    "                pred['predicted_demand'],\n",
    "                pred['confidence_interval_lower'],\n",
    "                pred['confidence_interval_upper'],\n",
    "                json.dumps(pred['factors'])\n",
    "            )\n",
    "            prediction_values.append(values)\n",
    "        \n",
    "        cursor.executemany(query, prediction_values)\n",
    "        updated_count = cursor.rowcount\n",
    "        \n",
    "        cursor.close()\n",
    "        return updated_count\n",
    "    \n",
    "    def run_forecasting_for_all_products(self, forecast_days=30):\n",
    "        \"\"\"Run forecasting for all active products\"\"\"\n",
    "        if not self.connect_database():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Get all active products\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT DISTINCT p.id, p.name, p.store_id, s.name as store_name\n",
    "                FROM products p\n",
    "                JOIN stores s ON p.store_id = s.id\n",
    "                WHERE p.status = 'active' AND s.status = 'active'\n",
    "                LIMIT 10\n",
    "            \"\"\")\n",
    "            \n",
    "            products = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            \n",
    "            total_predictions = 0\n",
    "            processed_products = 0\n",
    "            \n",
    "            print(f\"üîÑ Processing {len(products)} products...\")\n",
    "            \n",
    "            for product_id, product_name, store_id, store_name in products:\n",
    "                # Prepare historical data\n",
    "                historical_data = self.prepare_forecasting_data(product_id, store_id)\n",
    "                \n",
    "                if historical_data is not None:\n",
    "                    # Generate predictions\n",
    "                    predictions = self.simple_demand_forecast(historical_data, forecast_days)\n",
    "                    \n",
    "                    if predictions:\n",
    "                        # Update database\n",
    "                        updated = self.update_demand_predictions(predictions, product_id, store_id)\n",
    "                        total_predictions += updated\n",
    "                        processed_products += 1\n",
    "                        \n",
    "                        print(f\"‚úÖ {product_name} ({store_name}): {len(predictions)} predictions\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è {product_name} ({store_name}): Failed to generate predictions\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {product_name} ({store_name}): Insufficient historical data\")\n",
    "            \n",
    "            # Commit changes\n",
    "            self.connection.commit()\n",
    "            \n",
    "            print(f\"\\nüéâ Forecasting complete!\")\n",
    "            print(f\"üìä Processed: {processed_products} products\")\n",
    "            print(f\"üìà Generated: {total_predictions} predictions\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during forecasting: {e}\")\n",
    "            self.connection.rollback()\n",
    "            return False\n",
    "        \n",
    "        finally:\n",
    "            self.disconnect_database()\n",
    "\n",
    "print(\"üöÄ Forecasting pipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Forecasting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the forecasting pipeline\n",
    "print(\"üöÄ Starting forecasting pipeline...\")\n",
    "\n",
    "pipeline = InvenCareForecastingPipeline(DB_CONFIG)\n",
    "\n",
    "# Run forecasting for all products (next 30 days)\n",
    "success = pipeline.run_forecasting_for_all_products(forecast_days=30)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Forecasting pipeline completed successfully!\")\n",
    "    print(\"üìä Check your forecasting dashboard for updated predictions\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Forecasting pipeline failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Forecasting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forecasting_results():\n",
    "    \"\"\"Load and visualize the generated forecasting results\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    # Load latest predictions\n",
    "    query = \"\"\"\n",
    "    SELECT dp.prediction_date, dp.predicted_demand, \n",
    "           dp.confidence_interval_lower, dp.confidence_interval_upper,\n",
    "           p.name as product_name, s.name as store_name,\n",
    "           p.category\n",
    "    FROM demand_predictions dp\n",
    "    JOIN products p ON dp.product_id = p.id\n",
    "    JOIN stores s ON dp.store_id = s.id\n",
    "    WHERE dp.prediction_date >= CURDATE()\n",
    "    AND dp.prediction_date <= DATE_ADD(CURDATE(), INTERVAL 14 DAY)\n",
    "    ORDER BY dp.prediction_date, p.name\n",
    "    LIMIT 200\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        predictions_df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        if len(predictions_df) == 0:\n",
    "            print(\"‚ö†Ô∏è No predictions found to visualize\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìä Visualizing {len(predictions_df)} predictions\")\n",
    "        \n",
    "        # Convert date column\n",
    "        predictions_df['prediction_date'] = pd.to_datetime(predictions_df['prediction_date'])\n",
    "        \n",
    "        # Get top 5 products by predicted demand\n",
    "        top_products = predictions_df.groupby('product_name')['predicted_demand'].sum().nlargest(5)\n",
    "        \n",
    "        # Create interactive plot with Plotly\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Top 5 Products - Demand Forecast', 'Daily Demand Predictions', \n",
    "                           'Demand by Category', 'Confidence Intervals'],\n",
    "            specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "        \n",
    "        # Plot 1: Top products forecast\n",
    "        for i, product in enumerate(top_products.index[:3]):  # Show top 3\n",
    "            product_data = predictions_df[predictions_df['product_name'] == product]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=product_data['prediction_date'],\n",
    "                    y=product_data['predicted_demand'],\n",
    "                    mode='lines+markers',\n",
    "                    name=product[:20],  # Truncate long names\n",
    "                    line=dict(width=2)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Plot 2: Daily total demand\n",
    "        daily_demand = predictions_df.groupby('prediction_date')['predicted_demand'].sum().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=daily_demand['prediction_date'],\n",
    "                y=daily_demand['predicted_demand'],\n",
    "                mode='lines+markers',\n",
    "                name='Total Daily Demand',\n",
    "                line=dict(color='red', width=3)\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Plot 3: Demand by category\n",
    "        category_demand = predictions_df.groupby('category')['predicted_demand'].sum().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=category_demand['category'],\n",
    "                y=category_demand['predicted_demand'],\n",
    "                name='Category Demand'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 4: Confidence intervals for first product\n",
    "        first_product = top_products.index[0]\n",
    "        product_data = predictions_df[predictions_df['product_name'] == first_product]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=product_data['prediction_date'],\n",
    "                y=product_data['confidence_interval_upper'],\n",
    "                fill=None,\n",
    "                mode='lines',\n",
    "                line_color='rgba(0,100,80,0)',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=product_data['prediction_date'],\n",
    "                y=product_data['confidence_interval_lower'],\n",
    "                fill='tonexty',\n",
    "                mode='lines',\n",
    "                line_color='rgba(0,100,80,0)',\n",
    "                name='Confidence Interval'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=product_data['prediction_date'],\n",
    "                y=product_data['predicted_demand'],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='blue'),\n",
    "                name=f'{first_product[:15]} Prediction'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"InvenCare Demand Forecasting Results\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nüìà Forecasting Summary:\")\n",
    "        print(f\"Total Predicted Demand (14 days): {predictions_df['predicted_demand'].sum():.0f} units\")\n",
    "        print(f\"Average Daily Demand: {daily_demand['predicted_demand'].mean():.1f} units\")\n",
    "        print(f\"Peak Demand Day: {daily_demand.loc[daily_demand['predicted_demand'].idxmax(), 'prediction_date'].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        print(\"\\nüèÜ Top 5 Products by Predicted Demand:\")\n",
    "        for i, (product, demand) in enumerate(top_products.items(), 1):\n",
    "            print(f\"{i}. {product}: {demand:.1f} units\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error visualizing results: {e}\")\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_forecasting_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Update Forecasting Models Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_forecasting_models():\n",
    "    \"\"\"Update the forecasting models table with current model info\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if not conn:\n",
    "        return False\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if table exists\n",
    "    cursor.execute(\"SHOW TABLES LIKE 'demand_forecasting_models'\")\n",
    "    if not cursor.fetchone():\n",
    "        print(\"‚ö†Ô∏è demand_forecasting_models table not found\")\n",
    "        return False\n",
    "    \n",
    "    # Model information\n",
    "    models = [\n",
    "        {\n",
    "            'model_name': 'SageMaker_Simple_Forecaster_v1',\n",
    "            'model_type': 'linear_regression',\n",
    "            'model_accuracy': 0.75,\n",
    "            'training_status': 'deployed'\n",
    "        },\n",
    "        {\n",
    "            'model_name': 'Moving_Average_Trend_Model',\n",
    "            'model_type': 'arima',\n",
    "            'model_accuracy': 0.68,\n",
    "            'training_status': 'deployed'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for model in models:\n",
    "            # Check available columns\n",
    "            cursor.execute(\"SHOW COLUMNS FROM demand_forecasting_models\")\n",
    "            columns = [row[0] for row in cursor.fetchall()]\n",
    "            \n",
    "            # Build insert query based on available columns\n",
    "            base_columns = ['model_name', 'model_type', 'model_accuracy', 'training_status']\n",
    "            values = [model['model_name'], model['model_type'], model['model_accuracy'], model['training_status']]\n",
    "            \n",
    "            placeholders = ', '.join(['%s'] * len(base_columns))\n",
    "            column_list = ', '.join(base_columns)\n",
    "            \n",
    "            query = f\"INSERT IGNORE INTO demand_forecasting_models ({column_list}) VALUES ({placeholders})\"\n",
    "            \n",
    "            cursor.execute(query, values)\n",
    "            \n",
    "            print(f\"‚úÖ Updated model: {model['model_name']}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"ÔøΩÔøΩ Forecasting models updated successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating models: {e}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "\n",
    "# Update models\n",
    "update_forecasting_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Schedule Daily Execution (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that can be called daily\n",
    "def daily_forecasting_update():\n",
    "    \"\"\"Function to run daily forecasting update\"\"\"\n",
    "    print(f\"üïê Starting daily forecasting update at {datetime.now()}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize pipeline\n",
    "        pipeline = InvenCareForecastingPipeline(DB_CONFIG)\n",
    "        \n",
    "        # Run forecasting\n",
    "        success = pipeline.run_forecasting_for_all_products(forecast_days=30)\n",
    "        \n",
    "        if success:\n",
    "            # Update models table\n",
    "            update_forecasting_models()\n",
    "            \n",
    "            print(f\"‚úÖ Daily forecasting update completed at {datetime.now()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Daily forecasting update failed at {datetime.now()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in daily update: {e}\")\n",
    "        return False\n",
    "\n",
    "# For manual testing - run the daily update now\n",
    "print(\"üß™ Testing daily update function...\")\n",
    "test_result = daily_forecasting_update()\n",
    "print(f\"Test result: {'‚úÖ Success' if test_result else '‚ùå Failed'}\")\n",
    "\n",
    "print(\"\\nüìã To schedule this notebook for daily execution:\")\n",
    "print(\"1. Save this notebook\")\n",
    "print(\"2. Use SageMaker Pipelines or EventBridge to trigger daily\")\n",
    "print(\"3. Or convert to a Python script and use CloudWatch Events + Lambda\")\n",
    "print(\"4. Or use SageMaker Processing Jobs with schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "def generate_forecasting_report():\n",
    "    \"\"\"Generate a summary report of forecasting results\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'report_type': 'Daily Forecasting Summary',\n",
    "        'data': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Count predictions\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM demand_predictions \n",
    "            WHERE prediction_date >= CURDATE()\n",
    "        \"\"\")\n",
    "        total_predictions = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get prediction summary\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT product_id) as products_forecasted,\n",
    "                COUNT(DISTINCT store_id) as stores_covered,\n",
    "                AVG(predicted_demand) as avg_predicted_demand,\n",
    "                SUM(predicted_demand) as total_predicted_demand\n",
    "            FROM demand_predictions dp\n",
    "            WHERE prediction_date BETWEEN CURDATE() AND DATE_ADD(CURDATE(), INTERVAL 7 DAY)\n",
    "        \"\"\")\n",
    "        \n",
    "        summary = cursor.fetchone()\n",
    "        \n",
    "        report['data'] = {\n",
    "            'total_predictions_generated': total_predictions,\n",
    "            'products_forecasted': summary[0],\n",
    "            'stores_covered': summary[1],\n",
    "            'avg_daily_demand': float(summary[2]) if summary[2] else 0,\n",
    "            'total_7day_demand': float(summary[3]) if summary[3] else 0\n",
    "        }\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Display report\n",
    "        print(\"\\nüìä FORECASTING SUMMARY REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Generated: {report['timestamp']}\")\n",
    "        print(f\"Total Predictions: {report['data']['total_predictions_generated']}\")\n",
    "        print(f\"Products Forecasted: {report['data']['products_forecasted']}\")\n",
    "        print(f\"Stores Covered: {report['data']['stores_covered']}\")\n",
    "        print(f\"Avg Daily Demand: {report['data']['avg_daily_demand']:.1f} units\")\n",
    "        print(f\"Total 7-Day Demand: {report['data']['total_7day_demand']:.0f} units\")\n",
    "        \n",
    "        # Save to S3 (optional)\n",
    "        report_json = json.dumps(report, indent=2)\n",
    "        \n",
    "        # Save locally\n",
    "        with open('forecasting_report.json', 'w') as f:\n",
    "            f.write(report_json)\n",
    "        \n",
    "        print(f\"\\nüíæ Report saved as forecasting_report.json\")\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating report: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate the report\n",
    "final_report = generate_forecasting_report()\n",
    "\n",
    "print(\"\\nüéâ SageMaker Forecasting Pipeline Complete!\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. Check your forecasting dashboard at /forecasting\")\n",
    "print(\"2. Set up daily scheduling using SageMaker Pipelines\")\n",
    "print(\"3. Monitor prediction accuracy over time\")\n",
    "print(\"4. Refine the forecasting model based on actual vs predicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
