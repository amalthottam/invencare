# ===============================================================================
# ğŸš€ InvenCare ML Analytics - Complete SageMaker Pipeline
# ===============================================================================
# 
# This script provides a complete database-connected ML pipeline for inventory 
# demand forecasting. It connects to your MySQL database, trains ML models,
# deploys to SageMaker endpoints, and integrates with your Lambda functions.
#
# USAGE:
# 1. Update DB_CONFIG with your database credentials
# 2. Run each section step by step
# 3. Copy the generated endpoint names to your Lambda function
#
# ===============================================================================

# ===============================================================================
# ğŸ“¦ STEP 1: Install Required Packages
# ===============================================================================
# Run this first if using conda_python3 kernel

import subprocess
import sys

def install_packages():
    """Install required packages for conda_python3 kernel"""
    packages = [
        'pymysql', 'pandas', 'numpy', 'scikit-learn', 
        'tensorflow==2.8.0', 'prophet', 'statsmodels', 
        'boto3', 'sagemaker', 'matplotlib', 'seaborn', 
        'plotly', 'joblib'
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
            print(f"âœ… Installed {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install {package}: {e}")
    
    print("âœ… Package installation completed!")
    print("âš ï¸  Please RESTART KERNEL after installation")

# Uncomment to install packages
# install_packages()

# ===============================================================================
# ğŸ”„ STEP 2: Setup and Database Connection
# ===============================================================================

import pandas as pd
import numpy as np
import pymysql
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.tensorflow import TensorFlow
from sagemaker.sklearn import SKLearn
import json
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

def setup_sagemaker():
    """Initialize SageMaker session"""
    try:
        role = get_execution_role()
        session = sagemaker.Session()
        bucket = session.default_bucket()
        region = session.boto_region_name
        
        print(f"ğŸ¯ SageMaker Setup Complete!")
        print(f"ğŸ“‹ SageMaker role: {role}")
        print(f"ğŸª£ S3 bucket: {bucket}")
        print(f"ğŸŒ Region: {region}")
        
        return role, session, bucket, region
    except Exception as e:
        print(f"âŒ SageMaker setup failed: {e}")
        return None, None, None, None

# Initialize SageMaker
role, session, bucket, region = setup_sagemaker()

# ===============================================================================
# ğŸ”§ STEP 3: Database Configuration
# ===============================================================================
# ğŸš¨ UPDATE THESE WITH YOUR ACTUAL DATABASE CREDENTIALS

DB_CONFIG = {
    'host': 'your-rds-endpoint.amazonaws.com',  # ğŸ”„ Replace with your RDS endpoint
    'user': 'admin',                           # ğŸ”„ Replace with your username
    'password': 'your-password',               # ğŸ”„ Replace with your password
    'database': 'inventory_management',        # Your database name
    'port': 3306
}

def get_db_connection():
    """Create database connection"""
    return pymysql.connect(**DB_CONFIG)

def test_database_connection():
    """Test database connection and display basic stats"""
    try:
        conn = get_db_connection()
        print("âœ… Database connection successful!")
        
        # Test query
        with conn.cursor() as cursor:
            cursor.execute("SELECT COUNT(*) as total FROM inventory_transactions")
            result = cursor.fetchone()
            print(f"ğŸ“Š Found {result[0]} transactions in database")
        
        conn.close()
        return True
    except Exception as e:
        print(f"âŒ Database connection failed: {e}")
        print("ğŸ”§ Please update the DB_CONFIG above with your credentials")
        return False

# Test database connection
db_connected = test_database_connection()

# ===============================================================================
# ğŸ“Š STEP 4: Data Extraction and Analysis
# ===============================================================================

def fetch_training_data(days_back=180):
    """Fetch transaction data for ML training"""
    if not db_connected:
        print("âŒ Database not connected. Cannot fetch data.")
        return None
    
    query = f"""
    SELECT 
        product_id,
        product_name,
        category,
        store_id,
        DATE(created_at) as date,
        SUM(CASE WHEN transaction_type = 'Sale' THEN quantity ELSE 0 END) as sales_quantity,
        SUM(CASE WHEN transaction_type = 'Sale' THEN total_amount ELSE 0 END) as sales_amount,
        AVG(unit_price) as avg_price,
        COUNT(*) as transaction_count
    FROM inventory_transactions 
    WHERE created_at >= DATE_SUB(NOW(), INTERVAL {days_back} DAY)
    GROUP BY product_id, store_id, DATE(created_at)
    ORDER BY product_id, store_id, date
    """
    
    try:
        conn = get_db_connection()
        df = pd.read_sql(query, conn)
        conn.close()
        
        # Convert date column
        df['date'] = pd.to_datetime(df['date'])
        
        print(f"âœ… Training data loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        print(f"ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
        print(f"ğŸ›ï¸ Unique products: {df['product_id'].nunique()}")
        print(f"ğŸª Unique stores: {df['store_id'].nunique()}")
        print(f"ğŸ“Š Categories: {df['category'].nunique()}")
        
        return df
    except Exception as e:
        print(f"âŒ Error fetching training data: {e}")
        return None

# Load training data
training_data = fetch_training_data()

def analyze_data(df):
    """Analyze and visualize the training data"""
    if df is None or len(df) == 0:
        print("âŒ No data to analyze")
        return
    
    print("ğŸ“ˆ Creating data visualizations...")
    
    # Create analysis plots
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Sales by category
    category_sales = df.groupby('category')['sales_amount'].sum().sort_values(ascending=False)
    category_sales.plot(kind='bar', ax=axes[0,0])
    axes[0,0].set_title('Sales by Category')
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # Daily sales trend
    daily_sales = df.groupby('date')['sales_amount'].sum()
    daily_sales.plot(ax=axes[0,1])
    axes[0,1].set_title('Daily Sales Trend')
    
    # Top products
    top_products = df.groupby('product_name')['sales_quantity'].sum().nlargest(10)
    top_products.plot(kind='barh', ax=axes[1,0])
    axes[1,0].set_title('Top 10 Products by Quantity')
    
    # Store performance
    store_sales = df.groupby('store_id')['sales_amount'].sum()
    store_sales.plot(kind='bar', ax=axes[1,1])
    axes[1,1].set_title('Sales by Store')
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… Data analysis complete!")

# Analyze data if available
if training_data is not None:
    analyze_data(training_data)

# ===============================================================================
# ğŸ¤– STEP 5: Prepare Data for ML Models
# ===============================================================================

def prepare_lstm_data(df, product_id, store_id, sequence_length=30):
    """Prepare time series data for LSTM model"""
    if df is None:
        return None, None, None
    
    product_data = df[
        (df['product_id'] == product_id) & 
        (df['store_id'] == store_id)
    ].copy()
    
    if len(product_data) < sequence_length + 10:
        return None, None, None
    
    # Create complete date range and fill missing values
    date_range = pd.date_range(
        start=product_data['date'].min(),
        end=product_data['date'].max(),
        freq='D'
    )
    
    complete_data = pd.DataFrame({'date': date_range})
    complete_data = complete_data.merge(product_data, on='date', how='left')
    complete_data['sales_quantity'] = complete_data['sales_quantity'].fillna(0)
    
    # Create sequences
    sales_data = complete_data['sales_quantity'].values
    
    X, y = [], []
    for i in range(sequence_length, len(sales_data)):
        X.append(sales_data[i-sequence_length:i])
        y.append(sales_data[i])
    
    return np.array(X), np.array(y), sales_data

def prepare_classification_features(df):
    """Prepare features for ABC classification"""
    if df is None:
        return None
    
    features = df.groupby(['product_id', 'store_id']).agg({
        'sales_quantity': ['sum', 'mean', 'std', 'count'],
        'sales_amount': ['sum', 'mean', 'std'],
        'avg_price': ['mean', 'std']
    }).round(4)
    
    # Flatten column names
    features.columns = ['_'.join(col).strip() for col in features.columns]
    
    # Add derived features
    features['revenue_per_transaction'] = features['sales_amount_sum'] / features['sales_quantity_count']
    features['price_volatility'] = features['avg_price_std'] / features['avg_price_mean']
    features['demand_volatility'] = features['sales_quantity_std'] / features['sales_quantity_mean']
    
    # Handle infinite and NaN values
    features = features.replace([np.inf, -np.inf], 0).fillna(0)
    
    return features

def test_data_preparation(df):
    """Test data preparation functions"""
    if df is None:
        print("âŒ No data available for testing")
        return
    
    # Get unique products
    unique_products = df[['product_id', 'store_id']].drop_duplicates()
    print(f"ğŸ¯ Found {len(unique_products)} unique product-store combinations")
    
    if len(unique_products) > 0:
        # Test LSTM data preparation
        sample_product = unique_products.iloc[0]
        print(f"ğŸ§ª Testing with sample product: {sample_product['product_id']} in {sample_product['store_id']}")
        
        X_sample, y_sample, sales_sample = prepare_lstm_data(
            df, sample_product['product_id'], sample_product['store_id']
        )
        
        if X_sample is not None:
            print(f"âœ… Sample LSTM data prepared - X shape: {X_sample.shape}, y shape: {y_sample.shape}")
            
            # Visualize sample data
            plt.figure(figsize=(12, 4))
            plt.plot(sales_sample)
            plt.title(f'Sales History - {sample_product["product_id"]}')
            plt.xlabel('Days')
            plt.ylabel('Sales Quantity')
            plt.grid(True, alpha=0.3)
            plt.show()
        else:
            print("âŒ Insufficient data for sample product")
    
    # Test classification features
    classification_features = prepare_classification_features(df)
    if classification_features is not None:
        print(f"ğŸ“Š Classification features prepared: {classification_features.shape}")
        print("\nğŸ“‹ Feature names:")
        for i, col in enumerate(classification_features.columns, 1):
            print(f"{i:2d}. {col}")

# Test data preparation
test_data_preparation(training_data)

# ===============================================================================
# ğŸš€ STEP 6: Upload Training Data to S3
# ===============================================================================

def upload_data_to_s3(df, features_df):
    """Upload training data to S3"""
    if df is None or bucket is None:
        print("âŒ Cannot upload data - missing data or S3 bucket")
        return False
    
    try:
        s3_client = boto3.client('s3')
        training_data_key = 'invencare/training-data/transactions.csv'
        features_data_key = 'invencare/training-data/features.csv'
        
        # Upload transaction data
        print("ğŸ“¤ Uploading training data to S3...")
        df.to_csv('/tmp/transactions.csv', index=False)
        s3_client.upload_file('/tmp/transactions.csv', bucket, training_data_key)
        print(f"âœ… Training data uploaded to s3://{bucket}/{training_data_key}")
        
        # Upload features data
        if features_df is not None:
            features_df.to_csv('/tmp/features.csv')
            s3_client.upload_file('/tmp/features.csv', bucket, features_data_key)
            print(f"âœ… Features data uploaded to s3://{bucket}/{features_data_key}")
        
        print(f"\nğŸ¯ S3 Upload Summary:")
        print(f"ğŸ“Š Transaction records: {len(df)}")
        print(f"ğŸ”¢ Feature records: {len(features_df) if features_df is not None else 0}")
        print(f"ğŸª£ S3 Bucket: {bucket}")
        
        return True
    except Exception as e:
        print(f"âŒ Error uploading to S3: {e}")
        return False

# Upload data to S3
if training_data is not None:
    classification_features = prepare_classification_features(training_data)
    upload_success = upload_data_to_s3(training_data, classification_features)

# ===============================================================================
# ğŸ§  STEP 7: Create LSTM Training Script
# ===============================================================================

def create_lstm_training_script():
    """Create optimized LSTM training script for SageMaker"""
    
    lstm_script = '''
import argparse
import os
import json
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i])
        y.append(data[i])
    return np.array(X), np.array(y)

def build_lstm_model(seq_length, n_features=1):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(seq_length, n_features)),
        BatchNormalization(),
        Dropout(0.3),
        
        LSTM(32, return_sequences=True),
        BatchNormalization(),
        Dropout(0.3),
        
        LSTM(16),
        BatchNormalization(),
        Dropout(0.2),
        
        Dense(8, activation='relu'),
        Dense(1, activation='linear')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='huber',  # More robust to outliers
        metrics=['mae', 'mse']
    )
    return model

def prepare_training_data(data_path, seq_length=30):
    # Load data
    df = pd.read_csv(data_path)
    df['date'] = pd.to_datetime(df['date'])
    
    # Aggregate daily sales across all products/stores
    daily_sales = df.groupby('date')['sales_quantity'].sum().sort_index()
    
    # Fill missing dates
    idx = pd.date_range(daily_sales.index.min(), daily_sales.index.max(), freq='D')
    daily_sales = daily_sales.reindex(idx, fill_value=0)
    
    return daily_sales.values

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=32)
    parser.add_argument('--seq-length', type=int, default=30)
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting LSTM training with parameters:")
    print(f"   Epochs: {args.epochs}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Sequence length: {args.seq_length}")
    
    # Prepare data
    sales_data = prepare_training_data(os.path.join(args.train, 'transactions.csv'), args.seq_length)
    print(f"ğŸ“Š Loaded {len(sales_data)} days of sales data")
    
    # Scale data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(sales_data.reshape(-1, 1)).flatten()
    
    # Create sequences
    X, y = create_sequences(scaled_data, args.seq_length)
    print(f"ğŸ“ˆ Created {len(X)} training sequences")
    
    # Split data
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # Reshape for LSTM
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
    
    print(f"ğŸ“‹ Training set: {X_train.shape}, Test set: {X_test.shape}")
    
    # Build model
    model = build_lstm_model(args.seq_length)
    print(f"ğŸ§  Model architecture:")
    model.summary()
    
    # Callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=0.0001)
    ]
    
    # Train model
    print(f"ğŸ‹ï¸ Starting training...")
    history = model.fit(
        X_train, y_train,
        epochs=args.epochs,
        batch_size=args.batch_size,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # Evaluate
    train_predictions = model.predict(X_train)
    test_predictions = model.predict(X_test)
    
    train_mae = mean_absolute_error(y_train, train_predictions)
    test_mae = mean_absolute_error(y_test, test_predictions)
    train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
    
    print(f"\\nğŸ“Š Training Results:")
    print(f"   Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}")
    print(f"   Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}")
    
    # Save model
    model.save(os.path.join(args.model_dir, 'lstm_model.h5'))
    joblib.dump(scaler, os.path.join(args.model_dir, 'scaler.pkl'))
    
    # Save metadata
    metadata = {
        'model_type': 'lstm_demand_forecasting',
        'version': '2.0',
        'seq_length': args.seq_length,
        'train_mae': float(train_mae),
        'test_mae': float(test_mae),
        'train_rmse': float(train_rmse),
        'test_rmse': float(test_rmse),
        'training_samples': int(len(X_train)),
        'test_samples': int(len(X_test)),
        'epochs_trained': len(history.history['loss']),
        'created_at': pd.Timestamp.now().isoformat()
    }
    
    with open(os.path.join(args.model_dir, 'model_metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… Model saved successfully!")
    print(f"   Location: {args.model_dir}")
    print(f"   Files: lstm_model.h5, scaler.pkl, model_metadata.json")
'''
    
    # Save training script
    with open('invencare_lstm_training.py', 'w') as f:
        f.write(lstm_script)
    
    print("âœ… LSTM training script created: invencare_lstm_training.py")
    print("ğŸ¯ Features:")
    print("   â€¢ Advanced LSTM architecture with BatchNormalization")
    print("   â€¢ Robust loss function (Huber) for outlier handling")
    print("   â€¢ Early stopping and learning rate reduction")
    print("   â€¢ Comprehensive metadata saving")
    print("   â€¢ Optimized for your inventory data")

# Create LSTM training script
create_lstm_training_script()

# ===============================================================================
# ğŸš€ STEP 8: Train LSTM Model with SageMaker
# ===============================================================================

def train_lstm_model():
    """Train LSTM model using SageMaker"""
    if not upload_success or role is None:
        print("âŒ Cannot train model - data upload failed or SageMaker not configured")
        return None
    
    # Create TensorFlow estimator
    lstm_estimator = TensorFlow(
        entry_point='invencare_lstm_training.py',
        role=role,
        instance_count=1,
        instance_type='ml.m5.xlarge',  # Upgraded for better performance
        framework_version='2.8.0',
        py_version='py39',
        hyperparameters={
            'epochs': 100,
            'batch-size': 32,
            'seq-length': 30
        },
        max_run=3600,  # 1 hour timeout
        base_job_name='invencare-lstm-training'
    )
    
    print("ğŸš€ Starting LSTM model training...")
    print("ğŸ“‹ Training configuration:")
    print(f"   Instance: ml.m5.xlarge")
    print(f"   Framework: TensorFlow 2.8.0")
    print(f"   Data: s3://{bucket}/invencare/training-data/")
    
    # Start training
    training_input = f's3://{bucket}/invencare/training-data/'
    
    try:
        lstm_estimator.fit({'training': training_input}, wait=True)
        print("\nâœ… LSTM model training completed successfully!")
        print(f"ğŸ“Š Model artifacts location: {lstm_estimator.model_data}")
        
        # Store training job info
        training_job_name = lstm_estimator.latest_training_job.job_name
        print(f"ğŸ·ï¸ Training job name: {training_job_name}")
        
        return lstm_estimator
        
    except Exception as e:
        print(f"âŒ Training failed: {str(e)}")
        print("ğŸ”§ Check the training logs in SageMaker console for details")
        return None

# Train the model (uncomment when ready)
# lstm_estimator = train_lstm_model()

# ===============================================================================
# ğŸ¯ STEP 9: Create Inference Script and Deploy Model
# ===============================================================================

def create_inference_script():
    """Create comprehensive inference script"""
    
    inference_script = '''
import json
import numpy as np
import tensorflow as tf
import joblib
import os
from datetime import datetime
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def model_fn(model_dir):
    """Load model and preprocessing components"""
    try:
        # Load TensorFlow model
        model = tf.keras.models.load_model(os.path.join(model_dir, 'lstm_model.h5'))
        
        # Load scaler
        scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
        
        # Load metadata
        with open(os.path.join(model_dir, 'model_metadata.json'), 'r') as f:
            metadata = json.load(f)
        
        logger.info(f"Model loaded successfully: {metadata['model_type']} v{metadata['version']}")
        
        return {
            'model': model,
            'scaler': scaler,
            'metadata': metadata
        }
    except Exception as e:
        logger.error(f"Error loading model: {str(e)}")
        raise e

def input_fn(request_body, request_content_type):
    """Parse input data"""
    if request_content_type == 'application/json':
        input_data = json.loads(request_body)
        return input_data
    else:
        raise ValueError(f"Unsupported content type: {request_content_type}")

def predict_fn(input_data, model_dict):
    """Generate demand predictions"""
    try:
        model = model_dict['model']
        scaler = model_dict['scaler']
        metadata = model_dict['metadata']
        
        # Extract input parameters
        historical_data = input_data.get('historical_data', [])
        forecast_days = input_data.get('forecast_days', 30)
        product_id = input_data.get('product_id', 'unknown')
        store_id = input_data.get('store_id', 'unknown')
        
        if len(historical_data) == 0:
            return {
                'error': 'No historical data provided',
                'product_id': product_id,
                'store_id': store_id
            }
        
        # Convert to numpy array and ensure proper shape
        historical_array = np.array(historical_data, dtype=np.float32)
        
        # Scale input data
        scaled_data = scaler.transform(historical_array.reshape(-1, 1)).flatten()
        
        seq_length = metadata['seq_length']
        
        # Prepare sequence for prediction
        if len(scaled_data) < seq_length:
            # Pad with zeros if insufficient data
            padded_data = np.zeros(seq_length)
            padded_data[-len(scaled_data):] = scaled_data
            current_sequence = padded_data
        else:
            current_sequence = scaled_data[-seq_length:]
        
        # Generate predictions
        predictions = []
        sequence = current_sequence.copy()
        
        for day in range(forecast_days):
            # Reshape for model input
            model_input = sequence.reshape(1, seq_length, 1)
            
            # Predict next value
            next_pred = model.predict(model_input, verbose=0)[0][0]
            
            # Ensure non-negative prediction
            next_pred = max(0.0, next_pred)
            
            predictions.append(float(next_pred))
            
            # Update sequence for next prediction
            sequence = np.append(sequence[1:], next_pred)
        
        # Inverse transform predictions
        predictions_array = np.array(predictions).reshape(-1, 1)
        actual_predictions = scaler.inverse_transform(predictions_array).flatten()
        
        # Ensure non-negative predictions
        actual_predictions = np.maximum(actual_predictions, 0)
        
        # Calculate confidence intervals (simple approach)
        confidence_factor = 0.2  # 20% confidence band
        confidence_lower = actual_predictions * (1 - confidence_factor)
        confidence_upper = actual_predictions * (1 + confidence_factor)
        
        # Calculate basic statistics
        mean_prediction = float(np.mean(actual_predictions))
        total_prediction = float(np.sum(actual_predictions))
        
        # Build response
        response = {
            'success': True,
            'product_id': product_id,
            'store_id': store_id,
            'forecast_horizon': forecast_days,
            'predictions': actual_predictions.tolist(),
            'confidence_lower': confidence_lower.tolist(),
            'confidence_upper': confidence_upper.tolist(),
            'model_accuracy': metadata.get('test_mae', 0.0),
            'model_version': metadata.get('version', '1.0'),
            'statistics': {
                'mean_daily_demand': mean_prediction,
                'total_forecast_demand': total_prediction,
                'max_daily_demand': float(np.max(actual_predictions)),
                'min_daily_demand': float(np.min(actual_predictions))
            },
            'metadata': {
                'historical_data_points': len(historical_data),
                'sequence_length': seq_length,
                'prediction_timestamp': datetime.now().isoformat()
            }
        }
        
        logger.info(f"Prediction generated for {product_id}-{store_id}: {forecast_days} days")
        return response
        
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'product_id': input_data.get('product_id', 'unknown'),
            'store_id': input_data.get('store_id', 'unknown')
        }

def output_fn(prediction, content_type):
    """Format output"""
    if content_type == 'application/json':
        return json.dumps(prediction)
    else:
        raise ValueError(f"Unsupported content type: {content_type}")
'''
    
    # Save inference script
    with open('invencare_inference.py', 'w') as f:
        f.write(inference_script)
    
    print("âœ… Inference script created: invencare_inference.py")
    print("ğŸ¯ Features:")
    print("   â€¢ Comprehensive error handling")
    print("   â€¢ Detailed prediction metadata")
    print("   â€¢ Confidence intervals")
    print("   â€¢ Statistical summaries")
    print("   â€¢ Product/store tracking")

# Create inference script
create_inference_script()

def deploy_model(estimator):
    """Deploy model to SageMaker endpoint"""
    if estimator is None:
        print("âŒ No trained estimator available for deployment")
        return None
    
    endpoint_name = f'invencare-demand-forecasting-{datetime.now().strftime("%Y%m%d%H%M")}'
    
    print(f"ğŸš€ Deploying model to endpoint: {endpoint_name}")
    print("â³ This may take 5-10 minutes...")
    
    try:
        predictor = estimator.deploy(
            initial_instance_count=1,
            instance_type='ml.m5.large',
            endpoint_name=endpoint_name,
            wait=True
        )
        
        print("\nâœ… Model deployed successfully!")
        print(f"ğŸ¯ Endpoint name: {endpoint_name}")
        print(f"ğŸ“ Endpoint ARN: {predictor.endpoint_name}")
        
        # Store endpoint info for later use
        endpoint_info = {
            'endpoint_name': endpoint_name,
            'instance_type': 'ml.m5.large',
            'created_at': datetime.now().isoformat(),
            'model_data': estimator.model_data,
            'training_job': estimator.latest_training_job.job_name
        }
        
        with open('/tmp/endpoint_info.json', 'w') as f:
            json.dump(endpoint_info, f, indent=2)
        
        print("\nğŸ“‹ Important: Save this endpoint name for your Lambda function:")
        print(f"LSTM_ENDPOINT_NAME={endpoint_name}")
        
        return predictor, endpoint_name
        
    except Exception as e:
        print(f"âŒ Deployment failed: {str(e)}")
        print("ğŸ”§ Check the SageMaker console for deployment details")
        return None, None

# Deploy model (uncomment when ready)
# predictor, endpoint_name = deploy_model(lstm_estimator)

# ===============================================================================
# ğŸ§ª STEP 10: Test the Deployed Model
# ===============================================================================

def test_model_prediction(predictor, df):
    """Test prediction with real data"""
    if predictor is None or df is None:
        print("âŒ No predictor or data available for testing")
        return None
    
    print("ğŸ§ª Testing deployed model with sample data...")
    
    # Get sample product data
    unique_products = df[['product_id', 'store_id']].drop_duplicates()
    if len(unique_products) == 0:
        print("âŒ No products available for testing")
        return None
    
    sample_product = unique_products.iloc[0]
    product_id = sample_product['product_id']
    store_id = sample_product['store_id']
    
    # Get historical data for this product
    historical_sales = df[
        (df['product_id'] == product_id) & 
        (df['store_id'] == store_id)
    ]['sales_quantity'].values[-30:]  # Last 30 days
    
    test_data = {
        'historical_data': historical_sales.tolist(),
        'forecast_days': 14,
        'product_id': product_id,
        'store_id': store_id
    }
    
    print(f"ğŸ“Š Testing with product: {product_id} from {store_id}")
    print(f"ğŸ“ˆ Historical data points: {len(test_data['historical_data'])}")
    print(f"ğŸ”® Forecast horizon: {test_data['forecast_days']} days")
    
    try:
        # Make prediction
        result = predictor.predict(test_data)
        
        print("\nâœ… Prediction successful!")
        print("\nğŸ“‹ Prediction Results:")
        print(json.dumps(result, indent=2))
        
        return result
        
    except Exception as e:
        print(f"âŒ Prediction failed: {str(e)}")
        return None

# Test model (uncomment when model is deployed)
# test_result = test_model_prediction(predictor, training_data)

# ===============================================================================
# ğŸ’¾ STEP 11: Database Storage Functions
# ===============================================================================

def store_prediction_in_db(product_id, store_id, prediction_result):
    """Store prediction results in database"""
    if not db_connected:
        print("âŒ Database not connected")
        return False
    
    try:
        conn = get_db_connection()
        
        # Create predictions table if not exists
        create_table_query = """
        CREATE TABLE IF NOT EXISTS demand_predictions (
            id INT AUTO_INCREMENT PRIMARY KEY,
            product_id VARCHAR(50) NOT NULL,
            store_id VARCHAR(50) NOT NULL,
            model_type VARCHAR(50) NOT NULL,
            forecast_days INT NOT NULL,
            predictions JSON NOT NULL,
            confidence_lower JSON,
            confidence_upper JSON,
            model_accuracy DECIMAL(5,4),
            model_version VARCHAR(20),
            statistics JSON,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_product_store (product_id, store_id),
            INDEX idx_created_at (created_at),
            INDEX idx_model_type (model_type)
        )
        """
        
        with conn.cursor() as cursor:
            cursor.execute(create_table_query)
            
            # Insert prediction
            insert_query = """
            INSERT INTO demand_predictions 
            (product_id, store_id, model_type, forecast_days, predictions, 
             confidence_lower, confidence_upper, model_accuracy, model_version, statistics)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            values = (
                product_id,
                store_id,
                'lstm',
                prediction_result.get('forecast_horizon', 30),
                json.dumps(prediction_result.get('predictions', [])),
                json.dumps(prediction_result.get('confidence_lower', [])),
                json.dumps(prediction_result.get('confidence_upper', [])),
                prediction_result.get('model_accuracy'),
                prediction_result.get('model_version', '1.0'),
                json.dumps(prediction_result.get('statistics', {}))
            )
            
            cursor.execute(insert_query, values)
            conn.commit()
        
        conn.close()
        print(f"âœ… Prediction stored for {product_id} in {store_id}")
        return True
        
    except Exception as e:
        print(f"âŒ Error storing prediction: {str(e)}")
        return False

def get_prediction_summary():
    """Get summary of all stored predictions"""
    if not db_connected:
        print("âŒ Database not connected")
        return None, None
    
    try:
        conn = get_db_connection()
        
        # Get recent predictions summary
        summary_query = """
        SELECT 
            product_id,
            store_id,
            model_type,
            forecast_days,
            model_accuracy,
            model_version,
            created_at,
            JSON_EXTRACT(statistics, '$.mean_daily_demand') as avg_daily_demand,
            JSON_EXTRACT(statistics, '$.total_forecast_demand') as total_demand
        FROM demand_predictions 
        ORDER BY created_at DESC 
        LIMIT 20
        """
        
        summary_df = pd.read_sql(summary_query, conn)
        
        # Get overall statistics
        stats_query = """
        SELECT 
            COUNT(*) as total_predictions,
            COUNT(DISTINCT product_id) as unique_products,
            COUNT(DISTINCT store_id) as unique_stores,
            AVG(model_accuracy) as avg_model_accuracy,
            MIN(created_at) as first_prediction,
            MAX(created_at) as latest_prediction
        FROM demand_predictions
        """
        
        stats_df = pd.read_sql(stats_query, conn)
        conn.close()
        
        return summary_df, stats_df
        
    except Exception as e:
        print(f"âŒ Error getting prediction summary: {str(e)}")
        return None, None

# ===============================================================================
# ğŸ“Š STEP 12: Final Setup Instructions
# ===============================================================================

def generate_setup_instructions():
    """Generate final setup instructions"""
    print("ğŸ¯ SETUP COMPLETE! Here's what you need to do next:")
    print("\n" + "="*60)
    
    print("\nğŸ“‹ 1. UPDATE YOUR LAMBDA FUNCTION ENVIRONMENT VARIABLES:")
    print("   # Replace 'your-endpoint-name' with the actual endpoint name from Step 8")
    print("   LSTM_ENDPOINT_NAME=your-endpoint-name")
    print(f"   AWS_REGION={region}")
    print(f"   DB_HOST={DB_CONFIG['host']}")
    print(f"   DB_USER={DB_CONFIG['user']}")
    print(f"   DB_PASSWORD={DB_CONFIG['password']}")
    print(f"   DB_NAME={DB_CONFIG['database']}")
    
    print("\nğŸ“‹ 2. UPDATE YOUR EXPRESS SERVER .ENV FILE:")
    print("   LAMBDA_ML_ANALYTICS_FUNCTION=invencare-ml-analytics")
    print(f"   AWS_REGION={region}")
    
    print("\nğŸ“‹ 3. DEPLOY YOUR LAMBDA FUNCTION:")
    print("   Run: ./deploy-lambda-ml-analytics.sh")
    print("   (Use the deployment script provided earlier)")
    
    print("\nğŸ“‹ 4. TEST YOUR SETUP:")
    print("   â€¢ Go to your Forecasting page")
    print("   â€¢ Click 'Refresh & Predict' button")
    print("   â€¢ Check browser console for API calls")
    print("   â€¢ Verify new predictions appear")
    
    print("\nğŸ“‹ 5. AVAILABLE API ENDPOINTS:")
    print("   POST /api/ml/analytics - Main ML operations")
    print("   GET  /api/ml/predictions/:product_id/:store_id - Get predictions")
    print("   GET  /api/ml/dashboard - Dashboard data")
    print("   GET  /api/ml/health - Health check")
    
    print("\nğŸ“Š 6. DATABASE TABLES CREATED:")
    print("   â€¢ demand_predictions - Stores forecast results")
    print("   â€¢ product_classifications - Stores ABC analysis (coming soon)")
    
    print("\n" + "="*60)
    print("ğŸš€ Your ML-powered inventory system is ready!")
    print("ğŸ¯ Features enabled:")
    print("   âœ… Real-time demand forecasting")
    print("   âœ… Database-connected training")
    print("   âœ… SageMaker deployment")
    print("   âœ… Lambda integration")
    print("   âœ… Express API endpoints")
    print("   âœ… Frontend refresh automation")
    print("="*60)
    
    print("\nğŸ‰ Congratulations! Your InvenCare ML Analytics system is ready to use!")

# Generate setup instructions
generate_setup_instructions()

# ===============================================================================
# ğŸ”§ UTILITY FUNCTIONS FOR TESTING
# ===============================================================================

def run_complete_pipeline():
    """Run the complete ML pipeline from start to finish"""
    print("ğŸš€ Running complete InvenCare ML pipeline...")
    
    # Step 1: Test database connection
    if not test_database_connection():
        print("âŒ Pipeline stopped - database connection failed")
        return
    
    # Step 2: Fetch and analyze data
    data = fetch_training_data()
    if data is None:
        print("âŒ Pipeline stopped - data fetch failed")
        return
    
    analyze_data(data)
    
    # Step 3: Prepare and upload data
    features = prepare_classification_features(data)
    if not upload_data_to_s3(data, features):
        print("âŒ Pipeline stopped - S3 upload failed")
        return
    
    # Step 4: Create training scripts
    create_lstm_training_script()
    create_inference_script()
    
    # Step 5: Train model (requires manual execution)
    print("\nğŸ‹ï¸ Next steps (manual execution required):")
    print("1. Run: lstm_estimator = train_lstm_model()")
    print("2. Run: predictor, endpoint_name = deploy_model(lstm_estimator)")
    print("3. Run: test_result = test_model_prediction(predictor, training_data)")
    print("4. Update Lambda function with the endpoint name")
    
    print("\nâœ… Pipeline preparation complete!")

# Uncomment to run the complete pipeline
# run_complete_pipeline()

print("\n" + "="*80)
print("ğŸ“‹ INVENCARE ML ANALYTICS - PYTHON SCRIPT READY")
print("="*80)
print("ğŸ“ This script contains all the code from the Jupyter notebook")
print("ğŸ”§ Update DB_CONFIG with your credentials and run step by step")
print("ğŸš€ Each function can be executed independently")
print("="*80)
