{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ InvenCare ML Analytics - SageMaker Pipeline\n",
    "\n",
    "**Complete database-connected ML pipeline for inventory demand forecasting**\n",
    "\n",
    "This notebook:\n",
    "1. Connects to your MySQL database\n",
    "2. Trains multiple ML models (LSTM, ARIMA, Prophet, Classification)\n",
    "3. Deploys models to SageMaker endpoints\n",
    "4. Makes predictions and stores results back in database\n",
    "5. Integrates with your Lambda function and Express API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Install Required Packages (conda_python3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages for conda_python3 kernel\n",
    "!pip install pymysql pandas numpy scikit-learn tensorflow==2.8.0 prophet statsmodels boto3 sagemaker matplotlib seaborn plotly joblib\n",
    "\n",
    "print(\"âœ… Package installation completed!\")\n",
    "print(\"âš ï¸  Please RESTART KERNEL after installation\")\n",
    "print(\"   Go to Kernel â†’ Restart Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 2: Setup and Database Connection\n",
    "**âš ï¸ Make sure to RESTART KERNEL before running this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.sklearn import SKLearn\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SageMaker session setup\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "\n",
    "print(f\"ğŸ¯ SageMaker Setup Complete!\")\n",
    "print(f\"ğŸ“‹ SageMaker role: {role}\")\n",
    "print(f\"ğŸª£ S3 bucket: {bucket}\")\n",
    "print(f\"ğŸŒ Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "# ğŸš¨ UPDATE THESE WITH YOUR ACTUAL DATABASE CREDENTIALS\n",
    "DB_CONFIG = {\n",
    "    'host': 'your-rds-endpoint.amazonaws.com',  # ğŸ”„ Replace with your RDS endpoint\n",
    "    'user': 'admin',                           # ğŸ”„ Replace with your username\n",
    "    'password': 'your-password',               # ğŸ”„ Replace with your password\n",
    "    'database': 'inventory_management',        # Your database name\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create database connection\"\"\"\n",
    "    return pymysql.connect(**DB_CONFIG)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_db_connection()\n",
    "    print(\"âœ… Database connection successful!\")\n",
    "    \n",
    "    # Test query\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT COUNT(*) as total FROM inventory_transactions\")\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"ğŸ“Š Found {result[0]} transactions in database\")\n",
    "    \n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Database connection failed: {e}\")\n",
    "    print(\"ğŸ”§ Please update the DB_CONFIG above with your credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 3: Data Extraction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data(days_back=180):\n",
    "    \"\"\"Fetch transaction data for ML training\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        product_name,\n",
    "        category,\n",
    "        store_id,\n",
    "        DATE(created_at) as date,\n",
    "        SUM(CASE WHEN transaction_type = 'Sale' THEN quantity ELSE 0 END) as sales_quantity,\n",
    "        SUM(CASE WHEN transaction_type = 'Sale' THEN total_amount ELSE 0 END) as sales_amount,\n",
    "        AVG(unit_price) as avg_price,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM inventory_transactions \n",
    "    WHERE created_at >= DATE_SUB(NOW(), INTERVAL {days_back} DAY)\n",
    "    GROUP BY product_id, store_id, DATE(created_at)\n",
    "    ORDER BY product_id, store_id, date\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert date column\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load training data\n",
    "print(\"ğŸ“¥ Fetching training data from database...\")\n",
    "training_data = fetch_training_data()\n",
    "print(f\"âœ… Training data loaded: {training_data.shape[0]} rows, {training_data.shape[1]} columns\")\n",
    "print(\"\\nğŸ“‹ First few rows:\")\n",
    "display(training_data.head())\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Data Summary:\")\n",
    "print(f\"ğŸ“… Date range: {training_data['date'].min()} to {training_data['date'].max()}\")\n",
    "print(f\"ğŸ›ï¸ Unique products: {training_data['product_id'].nunique()}\")\n",
    "print(f\"ğŸª Unique stores: {training_data['store_id'].nunique()}\")\n",
    "print(f\"ğŸ“Š Categories: {training_data['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Sales by Category', 'Daily Sales Trend', 'Top 10 Products', 'Store Performance'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Sales by category\n",
    "category_sales = training_data.groupby('category')['sales_amount'].sum().sort_values(ascending=False)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_sales.index, y=category_sales.values, name=\"Category Sales\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Daily sales trend\n",
    "daily_sales = training_data.groupby('date')['sales_amount'].sum().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sales['date'], y=daily_sales['sales_amount'], mode='lines+markers', name=\"Daily Sales\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Top products\n",
    "top_products = training_data.groupby('product_name')['sales_quantity'].sum().nlargest(10)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_products.values, y=top_products.index, orientation='h', name=\"Top Products\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Store performance\n",
    "store_sales = training_data.groupby('store_id')['sales_amount'].sum()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=store_sales.index, y=store_sales.values, name=\"Store Sales\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"ğŸ“Š InvenCare Analytics Dashboard\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Data analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 4: Prepare Data for ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(product_id, store_id, sequence_length=30):\n",
    "    \"\"\"Prepare time series data for LSTM model\"\"\"\n",
    "    product_data = training_data[\n",
    "        (training_data['product_id'] == product_id) & \n",
    "        (training_data['store_id'] == store_id)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(product_data) < sequence_length + 10:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Create complete date range and fill missing values\n",
    "    date_range = pd.date_range(\n",
    "        start=product_data['date'].min(),\n",
    "        end=product_data['date'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    complete_data = pd.DataFrame({'date': date_range})\n",
    "    complete_data = complete_data.merge(product_data, on='date', how='left')\n",
    "    complete_data['sales_quantity'] = complete_data['sales_quantity'].fillna(0)\n",
    "    \n",
    "    # Create sequences\n",
    "    sales_data = complete_data['sales_quantity'].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(sales_data)):\n",
    "        X.append(sales_data[i-sequence_length:i])\n",
    "        y.append(sales_data[i])\n",
    "    \n",
    "    return np.array(X), np.array(y), sales_data\n",
    "\n",
    "def prepare_classification_features():\n",
    "    \"\"\"Prepare features for ABC classification\"\"\"\n",
    "    features = training_data.groupby(['product_id', 'store_id']).agg({\n",
    "        'sales_quantity': ['sum', 'mean', 'std', 'count'],\n",
    "        'sales_amount': ['sum', 'mean', 'std'],\n",
    "        'avg_price': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    features.columns = ['_'.join(col).strip() for col in features.columns]\n",
    "    \n",
    "    # Add derived features\n",
    "    features['revenue_per_transaction'] = features['sales_amount_sum'] / features['sales_quantity_count']\n",
    "    features['price_volatility'] = features['avg_price_std'] / features['avg_price_mean']\n",
    "    features['demand_volatility'] = features['sales_quantity_std'] / features['sales_quantity_mean']\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    features = features.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Prepare sample data\n",
    "unique_products = training_data[['product_id', 'store_id']].drop_duplicates()\n",
    "print(f\"ğŸ¯ Found {len(unique_products)} unique product-store combinations\")\n",
    "\n",
    "# Show sample LSTM data preparation\n",
    "sample_product = unique_products.iloc[0]\n",
    "print(f\"\\nğŸ§ª Testing with sample product: {sample_product['product_id']} in {sample_product['store_id']}\")\n",
    "\n",
    "X_sample, y_sample, sales_sample = prepare_lstm_data(sample_product['product_id'], sample_product['store_id'])\n",
    "\n",
    "if X_sample is not None:\n",
    "    print(f\"âœ… Sample LSTM data prepared - X shape: {X_sample.shape}, y shape: {y_sample.shape}\")\n",
    "    \n",
    "    # Visualize sample data\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(sales_sample)\n",
    "    plt.title(f'Sales History - {sample_product[\"product_id\"]}')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Sales Quantity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ Insufficient data for sample product\")\n",
    "\n",
    "# Show classification features\n",
    "classification_features = prepare_classification_features()\n",
    "print(f\"\\nğŸ“Š Classification features prepared: {classification_features.shape}\")\n",
    "print(\"\\nğŸ“‹ Feature names:\")\n",
    "for i, col in enumerate(classification_features.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 5: Upload Training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data to S3\n",
    "s3_client = boto3.client('s3')\n",
    "training_data_key = 'invencare/training-data/transactions.csv'\n",
    "features_data_key = 'invencare/training-data/features.csv'\n",
    "\n",
    "# Upload transaction data\n",
    "print(\"ğŸ“¤ Uploading training data to S3...\")\n",
    "training_data.to_csv('/tmp/transactions.csv', index=False)\n",
    "s3_client.upload_file('/tmp/transactions.csv', bucket, training_data_key)\n",
    "print(f\"âœ… Training data uploaded to s3://{bucket}/{training_data_key}\")\n",
    "\n",
    "# Upload features data\n",
    "classification_features.to_csv('/tmp/features.csv')\n",
    "s3_client.upload_file('/tmp/features.csv', bucket, features_data_key)\n",
    "print(f\"âœ… Features data uploaded to s3://{bucket}/{features_data_key}\")\n",
    "\n",
    "print(\"\\nğŸ¯ S3 Upload Summary:\")\n",
    "print(f\"ğŸ“Š Transaction records: {len(training_data)}\")\n",
    "print(f\"ğŸ”¢ Feature records: {len(classification_features)}\")\n",
    "print(f\"ğŸª£ S3 Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 6: Create LSTM Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM training script optimized for your data\n",
    "lstm_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_lstm_model(seq_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(seq_length, n_features)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(32, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(16),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='huber',  # More robust to outliers\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def prepare_training_data(data_path, seq_length=30):\n",
    "    # Load data\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Aggregate daily sales across all products/stores\n",
    "    daily_sales = df.groupby('date')['sales_quantity'].sum().sort_index()\n",
    "    \n",
    "    # Fill missing dates\n",
    "    idx = pd.date_range(daily_sales.index.min(), daily_sales.index.max(), freq='D')\n",
    "    daily_sales = daily_sales.reindex(idx, fill_value=0)\n",
    "    \n",
    "    return daily_sales.values\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--epochs', type=int, default=100)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    parser.add_argument('--seq-length', type=int, default=30)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\"ğŸš€ Starting LSTM training with parameters:\")\n",
    "    print(f\"   Epochs: {args.epochs}\")\n",
    "    print(f\"   Batch size: {args.batch_size}\")\n",
    "    print(f\"   Sequence length: {args.seq_length}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    sales_data = prepare_training_data(os.path.join(args.train, 'transactions.csv'), args.seq_length)\n",
    "    print(f\"ğŸ“Š Loaded {len(sales_data)} days of sales data\")\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(sales_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data, args.seq_length)\n",
    "    print(f\"ğŸ“ˆ Created {len(X)} training sequences\")\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Reshape for LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    print(f\"ğŸ“‹ Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(args.seq_length)\n",
    "    print(f\"ğŸ§  Model architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=0.0001)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"ğŸ‹ï¸ Starting training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Training Results:\")\n",
    "    print(f\"   Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"   Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(args.model_dir, 'lstm_model.h5'))\n",
    "    joblib.dump(scaler, os.path.join(args.model_dir, 'scaler.pkl'))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_type': 'lstm_demand_forecasting',\n",
    "        'version': '2.0',\n",
    "        'seq_length': args.seq_length,\n",
    "        'train_mae': float(train_mae),\n",
    "        'test_mae': float(test_mae),\n",
    "        'train_rmse': float(train_rmse),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'training_samples': int(len(X_train)),\n",
    "        'test_samples': int(len(X_test)),\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'created_at': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(args.model_dir, 'model_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Model saved successfully!\")\n",
    "    print(f\"   Location: {args.model_dir}\")\n",
    "    print(f\"   Files: lstm_model.h5, scaler.pkl, model_metadata.json\")\n",
    "\"\"\"\n",
    "\n",
    "# Save training script\n",
    "with open('invencare_lstm_training.py', 'w') as f:\n",
    "    f.write(lstm_script)\n",
    "\n",
    "print(\"âœ… LSTM training script created: invencare_lstm_training.py\")\n",
    "print(\"ğŸ¯ Features:\")\n",
    "print(\"   â€¢ Advanced LSTM architecture with BatchNormalization\")\n",
    "print(\"   â€¢ Robust loss function (Huber) for outlier handling\")\n",
    "print(\"   â€¢ Early stopping and learning rate reduction\")\n",
    "print(\"   â€¢ Comprehensive metadata saving\")\n",
    "print(\"   â€¢ Optimized for your inventory data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 7: Train LSTM Model with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow estimator for LSTM training\n",
    "lstm_estimator = TensorFlow(\n",
    "    entry_point='invencare_lstm_training.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',  # Upgraded for better performance\n",
    "    framework_version='2.8.0',\n",
    "    py_version='py39',\n",
    "    hyperparameters={\n",
    "        'epochs': 100,\n",
    "        'batch-size': 32,\n",
    "        'seq-length': 30\n",
    "    },\n",
    "    max_run=3600,  # 1 hour timeout\n",
    "    base_job_name='invencare-lstm-training'\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Starting LSTM model training...\")\n",
    "print(\"ğŸ“‹ Training configuration:\")\n",
    "print(f\"   Instance: ml.m5.xlarge\")\n",
    "print(f\"   Framework: TensorFlow 2.8.0\")\n",
    "print(f\"   Data: s3://{bucket}/invencare/training-data/\")\n",
    "\n",
    "# Start training\n",
    "training_input = f's3://{bucket}/invencare/training-data/'\n",
    "\n",
    "try:\n",
    "    lstm_estimator.fit({'training': training_input}, wait=True)\n",
    "    print(\"\\nâœ… LSTM model training completed successfully!\")\n",
    "    print(f\"ğŸ“Š Model artifacts location: {lstm_estimator.model_data}\")\n",
    "    \n",
    "    # Store training job info\n",
    "    training_job_name = lstm_estimator.latest_training_job.job_name\n",
    "    print(f\"ğŸ·ï¸ Training job name: {training_job_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {str(e)}\")\n",
    "    print(\"ğŸ”§ Check the training logs in SageMaker console for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 8: Create Inference Script and Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive inference script\n",
    "inference_script = \"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \\\"\\\"\\\"Load model and preprocessing components\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Load TensorFlow model\n",
    "        model = tf.keras.models.load_model(os.path.join(model_dir, 'lstm_model.h5'))\n",
    "        \n",
    "        # Load scaler\n",
    "        scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(os.path.join(model_dir, 'model_metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        logger.info(f\\\"Model loaded successfully: {metadata['model_type']} v{metadata['version']}\\\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\\\"Error loading model: {str(e)}\\\")\n",
    "        raise e\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \\\"\\\"\\\"Parse input data\\\"\\\"\\\"\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        return input_data\n",
    "    else:\n",
    "        raise ValueError(f\\\"Unsupported content type: {request_content_type}\\\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \\\"\\\"\\\"Generate demand predictions\\\"\\\"\\\"\n",
    "    try:\n",
    "        model = model_dict['model']\n",
    "        scaler = model_dict['scaler']\n",
    "        metadata = model_dict['metadata']\n",
    "        \n",
    "        # Extract input parameters\n",
    "        historical_data = input_data.get('historical_data', [])\n",
    "        forecast_days = input_data.get('forecast_days', 30)\n",
    "        product_id = input_data.get('product_id', 'unknown')\n",
    "        store_id = input_data.get('store_id', 'unknown')\n",
    "        \n",
    "        if len(historical_data) == 0:\n",
    "            return {\n",
    "                'error': 'No historical data provided',\n",
    "                'product_id': product_id,\n",
    "                'store_id': store_id\n",
    "            }\n",
    "        \n",
    "        # Convert to numpy array and ensure proper shape\n",
    "        historical_array = np.array(historical_data, dtype=np.float32)\n",
    "        \n",
    "        # Scale input data\n",
    "        scaled_data = scaler.transform(historical_array.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        seq_length = metadata['seq_length']\n",
    "        \n",
    "        # Prepare sequence for prediction\n",
    "        if len(scaled_data) < seq_length:\n",
    "            # Pad with zeros if insufficient data\n",
    "            padded_data = np.zeros(seq_length)\n",
    "            padded_data[-len(scaled_data):] = scaled_data\n",
    "            current_sequence = padded_data\n",
    "        else:\n",
    "            current_sequence = scaled_data[-seq_length:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = []\n",
    "        sequence = current_sequence.copy()\n",
    "        \n",
    "        for day in range(forecast_days):\n",
    "            # Reshape for model input\n",
    "            model_input = sequence.reshape(1, seq_length, 1)\n",
    "            \n",
    "            # Predict next value\n",
    "            next_pred = model.predict(model_input, verbose=0)[0][0]\n",
    "            \n",
    "            # Ensure non-negative prediction\n",
    "            next_pred = max(0.0, next_pred)\n",
    "            \n",
    "            predictions.append(float(next_pred))\n",
    "            \n",
    "            # Update sequence for next prediction\n",
    "            sequence = np.append(sequence[1:], next_pred)\n",
    "        \n",
    "        # Inverse transform predictions\n",
    "        predictions_array = np.array(predictions).reshape(-1, 1)\n",
    "        actual_predictions = scaler.inverse_transform(predictions_array).flatten()\n",
    "        \n",
    "        # Ensure non-negative predictions\n",
    "        actual_predictions = np.maximum(actual_predictions, 0)\n",
    "        \n",
    "        # Calculate confidence intervals (simple approach)\n",
    "        confidence_factor = 0.2  # 20% confidence band\n",
    "        confidence_lower = actual_predictions * (1 - confidence_factor)\n",
    "        confidence_upper = actual_predictions * (1 + confidence_factor)\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        mean_prediction = float(np.mean(actual_predictions))\n",
    "        total_prediction = float(np.sum(actual_predictions))\n",
    "        \n",
    "        # Build response\n",
    "        response = {\n",
    "            'success': True,\n",
    "            'product_id': product_id,\n",
    "            'store_id': store_id,\n",
    "            'forecast_horizon': forecast_days,\n",
    "            'predictions': actual_predictions.tolist(),\n",
    "            'confidence_lower': confidence_lower.tolist(),\n",
    "            'confidence_upper': confidence_upper.tolist(),\n",
    "            'model_accuracy': metadata.get('test_mae', 0.0),\n",
    "            'model_version': metadata.get('version', '1.0'),\n",
    "            'statistics': {\n",
    "                'mean_daily_demand': mean_prediction,\n",
    "                'total_forecast_demand': total_prediction,\n",
    "                'max_daily_demand': float(np.max(actual_predictions)),\n",
    "                'min_daily_demand': float(np.min(actual_predictions))\n",
    "            },\n",
    "            'metadata': {\n",
    "                'historical_data_points': len(historical_data),\n",
    "                'sequence_length': seq_length,\n",
    "                'prediction_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\\\"Prediction generated for {product_id}-{store_id}: {forecast_days} days\\\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\\\"Prediction error: {str(e)}\\\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'product_id': input_data.get('product_id', 'unknown'),\n",
    "            'store_id': input_data.get('store_id', 'unknown')\n",
    "        }\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \\\"\\\"\\\"Format output\\\"\\\"\\\"\n",
    "    if content_type == 'application/json':\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f\\\"Unsupported content type: {content_type}\\\")\n",
    "\"\"\"\n",
    "\n",
    "# Save inference script\n",
    "with open('invencare_inference.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"âœ… Inference script created: invencare_inference.py\")\n",
    "print(\"ğŸ¯ Features:\")\n",
    "print(\"   â€¢ Comprehensive error handling\")\n",
    "print(\"   â€¢ Detailed prediction metadata\")\n",
    "print(\"   â€¢ Confidence intervals\")\n",
    "print(\"   â€¢ Statistical summaries\")\n",
    "print(\"   â€¢ Product/store tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to SageMaker endpoint\n",
    "endpoint_name = f'invencare-demand-forecasting-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    "\n",
    "print(f\"ğŸš€ Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"â³ This may take 5-10 minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = lstm_estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Model deployed successfully!\")\n",
    "    print(f\"ğŸ¯ Endpoint name: {endpoint_name}\")\n",
    "    print(f\"ğŸ“ Endpoint ARN: {predictor.endpoint_name}\")\n",
    "    \n",
    "    # Store endpoint info for later use\n",
    "    endpoint_info = {\n",
    "        'endpoint_name': endpoint_name,\n",
    "        'instance_type': 'ml.m5.large',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'model_data': lstm_estimator.model_data,\n",
    "        'training_job': lstm_estimator.latest_training_job.job_name\n",
    "    }\n",
    "    \n",
    "    with open('/tmp/endpoint_info.json', 'w') as f:\n",
    "        json.dump(endpoint_info, f, indent=2)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Important: Save this endpoint name for your Lambda function:\")\n",
    "    print(f\"LSTM_ENDPOINT_NAME={endpoint_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Deployment failed: {str(e)}\")\n",
    "    print(\"ğŸ”§ Check the SageMaker console for deployment details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 9: Test the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction with real data\n",
    "if 'predictor' in locals() and predictor is not None:\n",
    "    print(\"ğŸ§ª Testing deployed model with sample data...\")\n",
    "    \n",
    "    # Prepare test data from your actual dataset\n",
    "    sample_product = unique_products.iloc[0]\n",
    "    product_id = sample_product['product_id']\n",
    "    store_id = sample_product['store_id']\n",
    "    \n",
    "    # Get historical data for this product\n",
    "    historical_sales = training_data[\n",
    "        (training_data['product_id'] == product_id) & \n",
    "        (training_data['store_id'] == store_id)\n",
    "    ]['sales_quantity'].values[-30:]  # Last 30 days\n",
    "    \n",
    "    test_data = {\n",
    "        'historical_data': historical_sales.tolist(),\n",
    "        'forecast_days': 14,\n",
    "        'product_id': product_id,\n",
    "        'store_id': store_id\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š Testing with product: {product_id} from {store_id}\")\n",
    "    print(f\"ğŸ“ˆ Historical data points: {len(test_data['historical_data'])}\")\n",
    "    print(f\"ğŸ”® Forecast horizon: {test_data['forecast_days']} days\")\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = predictor.predict(test_data)\n",
    "        \n",
    "        print(\"\\nâœ… Prediction successful!\")\n",
    "        print(\"\\nğŸ“‹ Prediction Results:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Visualize prediction\n",
    "        if result.get('success', False):\n",
    "            predictions = result['predictions']\n",
    "            confidence_lower = result['confidence_lower']\n",
    "            confidence_upper = result['confidence_upper']\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Historical data\n",
    "            historical_days = list(range(-len(historical_sales), 0))\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=historical_days,\n",
    "                y=historical_sales,\n",
    "                mode='lines+markers',\n",
    "                name='Historical Sales',\n",
    "                line=dict(color='blue')\n",
    "            ))\n",
    "            \n",
    "            # Predictions\n",
    "            forecast_days = list(range(1, len(predictions) + 1))\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=forecast_days,\n",
    "                y=predictions,\n",
    "                mode='lines+markers',\n",
    "                name='Forecast',\n",
    "                line=dict(color='red', dash='dash')\n",
    "            ))\n",
    "            \n",
    "            # Confidence intervals\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=forecast_days + forecast_days[::-1],\n",
    "                y=confidence_upper + confidence_lower[::-1],\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(255,0,0,0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                name='Confidence Interval'\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f'ğŸ”® Demand Forecast - {product_id} ({store_id})',\n",
    "                xaxis_title='Days (negative = historical, positive = forecast)',\n",
    "                yaxis_title='Sales Quantity',\n",
    "                height=500\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Summary statistics\n",
    "            stats = result['statistics']\n",
    "            print(f\"\\nğŸ“Š Forecast Summary:\")\n",
    "            print(f\"   ğŸ“ˆ Mean daily demand: {stats['mean_daily_demand']:.2f} units\")\n",
    "            print(f\"   ğŸ“¦ Total forecast demand: {stats['total_forecast_demand']:.2f} units\")\n",
    "            print(f\"   ğŸ”º Max daily demand: {stats['max_daily_demand']:.2f} units\")\n",
    "            print(f\"   ğŸ”» Min daily demand: {stats['min_daily_demand']:.2f} units\")\n",
    "            print(f\"   ğŸ¯ Model accuracy (MAE): {result['model_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Prediction failed: {str(e)}\")\n",
    "        print(\"ğŸ”§ Check endpoint status and input data format\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No predictor available. Make sure the model was deployed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 10: Create Database Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_in_db(product_id, store_id, prediction_result):\n",
    "    \"\"\"Store prediction results in database\"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        \n",
    "        # Create predictions table if not exists\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS demand_predictions (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            product_id VARCHAR(50) NOT NULL,\n",
    "            store_id VARCHAR(50) NOT NULL,\n",
    "            model_type VARCHAR(50) NOT NULL,\n",
    "            forecast_days INT NOT NULL,\n",
    "            predictions JSON NOT NULL,\n",
    "            confidence_lower JSON,\n",
    "            confidence_upper JSON,\n",
    "            model_accuracy DECIMAL(5,4),\n",
    "            model_version VARCHAR(20),\n",
    "            statistics JSON,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            INDEX idx_product_store (product_id, store_id),\n",
    "            INDEX idx_created_at (created_at),\n",
    "            INDEX idx_model_type (model_type)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(create_table_query)\n",
    "            \n",
    "            # Insert prediction\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO demand_predictions \n",
    "            (product_id, store_id, model_type, forecast_days, predictions, \n",
    "             confidence_lower, confidence_upper, model_accuracy, model_version, statistics)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            values = (\n",
    "                product_id,\n",
    "                store_id,\n",
    "                'lstm',\n",
    "                prediction_result.get('forecast_horizon', 30),\n",
    "                json.dumps(prediction_result.get('predictions', [])),\n",
    "                json.dumps(prediction_result.get('confidence_lower', [])),\n",
    "                json.dumps(prediction_result.get('confidence_upper', [])),\n",
    "                prediction_result.get('model_accuracy'),\n",
    "                prediction_result.get('model_version', '1.0'),\n",
    "                json.dumps(prediction_result.get('statistics', {}))\n",
    "            )\n",
    "            \n",
    "            cursor.execute(insert_query, values)\n",
    "            conn.commit()\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error storing prediction: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def test_database_storage():\n",
    "    \"\"\"Test storing prediction in database\"\"\"\n",
    "    if 'result' in locals() and result.get('success', False):\n",
    "        print(\"ğŸ’¾ Testing database storage...\")\n",
    "        \n",
    "        success = store_prediction_in_db(product_id, store_id, result)\n",
    "        \n",
    "        if success:\n",
    "            print(\"âœ… Prediction stored in database successfully!\")\n",
    "            \n",
    "            # Verify storage\n",
    "            conn = get_db_connection()\n",
    "            \n",
    "            verify_query = \"\"\"\n",
    "            SELECT id, product_id, store_id, model_type, forecast_days, \n",
    "                   model_accuracy, model_version, created_at\n",
    "            FROM demand_predictions \n",
    "            WHERE product_id = %s AND store_id = %s\n",
    "            ORDER BY created_at DESC \n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            \n",
    "            stored_predictions = pd.read_sql(verify_query, conn, params=[product_id, store_id])\n",
    "            conn.close()\n",
    "            \n",
    "            print(\"\\nğŸ“‹ Stored prediction details:\")\n",
    "            display(stored_predictions)\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Failed to store prediction in database\")\n",
    "    else:\n",
    "        print(\"âŒ No valid prediction result to store\")\n",
    "\n",
    "# Test database storage\n",
    "test_database_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 11: Batch Predictions for Multiple Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_predictions(max_products=10):\n",
    "    \"\"\"Run predictions for multiple products and store in database\"\"\"\n",
    "    \n",
    "    if 'predictor' not in locals() or predictor is None:\n",
    "        print(\"âŒ No predictor available. Deploy the model first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸš€ Running batch predictions for up to {max_products} products...\")\n",
    "    \n",
    "    # Get products with sufficient data\n",
    "    product_data_counts = training_data.groupby(['product_id', 'store_id']).size().reset_index(name='data_points')\n",
    "    suitable_products = product_data_counts[product_data_counts['data_points'] >= 30].head(max_products)\n",
    "    \n",
    "    print(f\"ğŸ“Š Found {len(suitable_products)} products with sufficient data\")\n",
    "    \n",
    "    successful_predictions = 0\n",
    "    failed_predictions = 0\n",
    "    \n",
    "    for idx, product_row in suitable_products.iterrows():\n",
    "        try:\n",
    "            product_id = product_row['product_id']\n",
    "            store_id = product_row['store_id']\n",
    "            \n",
    "            print(f\"\\nğŸ”® Predicting for {product_id} in {store_id}...\")\n",
    "            \n",
    "            # Get historical data\n",
    "            historical_sales = training_data[\n",
    "                (training_data['product_id'] == product_id) & \n",
    "                (training_data['store_id'] == store_id)\n",
    "            ]['sales_quantity'].values[-30:]  # Last 30 days\n",
    "            \n",
    "            if len(historical_sales) < 10:\n",
    "                print(f\"   âš ï¸ Insufficient data ({len(historical_sales)} days), skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare prediction data\n",
    "            prediction_data = {\n",
    "                'historical_data': historical_sales.tolist(),\n",
    "                'forecast_days': 30,\n",
    "                'product_id': product_id,\n",
    "                'store_id': store_id\n",
    "            }\n",
    "            \n",
    "            # Make prediction\n",
    "            result = predictor.predict(prediction_data)\n",
    "            \n",
    "            if result.get('success', False):\n",
    "                # Store in database\n",
    "                if store_prediction_in_db(product_id, store_id, result):\n",
    "                    successful_predictions += 1\n",
    "                    stats = result.get('statistics', {})\n",
    "                    print(f\"   âœ… Success! Mean daily demand: {stats.get('mean_daily_demand', 0):.2f} units\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ Prediction made but database storage failed\")\n",
    "            else:\n",
    "                print(f\"   âŒ Prediction failed: {result.get('error', 'Unknown error')}\")\n",
    "                failed_predictions += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error processing {product_id}: {str(e)}\")\n",
    "            failed_predictions += 1\n",
    "        \n",
    "        # Small delay to avoid overwhelming the endpoint\n",
    "        import time\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Batch Prediction Summary:\")\n",
    "    print(f\"   âœ… Successful: {successful_predictions}\")\n",
    "    print(f\"   âŒ Failed: {failed_predictions}\")\n",
    "    print(f\"   ğŸ“ˆ Success rate: {(successful_predictions/(successful_predictions+failed_predictions)*100):.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'successful': successful_predictions,\n",
    "        'failed': failed_predictions,\n",
    "        'total_processed': len(suitable_products)\n",
    "    }\n",
    "\n",
    "# Run batch predictions\n",
    "batch_results = run_batch_predictions(5)  # Start with 5 products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 12: View Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of stored predictions\n",
    "def get_prediction_summary():\n",
    "    \"\"\"Get summary of all stored predictions\"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        \n",
    "        # Get recent predictions summary\n",
    "        summary_query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            store_id,\n",
    "            model_type,\n",
    "            forecast_days,\n",
    "            model_accuracy,\n",
    "            model_version,\n",
    "            created_at,\n",
    "            JSON_EXTRACT(statistics, '$.mean_daily_demand') as avg_daily_demand,\n",
    "            JSON_EXTRACT(statistics, '$.total_forecast_demand') as total_demand\n",
    "        FROM demand_predictions \n",
    "        ORDER BY created_at DESC \n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        summary_df = pd.read_sql(summary_query, conn)\n",
    "        \n",
    "        # Get overall statistics\n",
    "        stats_query = \"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_predictions,\n",
    "            COUNT(DISTINCT product_id) as unique_products,\n",
    "            COUNT(DISTINCT store_id) as unique_stores,\n",
    "            AVG(model_accuracy) as avg_model_accuracy,\n",
    "            MIN(created_at) as first_prediction,\n",
    "            MAX(created_at) as latest_prediction\n",
    "        FROM demand_predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        stats_df = pd.read_sql(stats_query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        return summary_df, stats_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting prediction summary: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Display prediction summary\n",
    "print(\"ğŸ“Š Getting prediction summary from database...\")\n",
    "summary_df, stats_df = get_prediction_summary()\n",
    "\n",
    "if summary_df is not None and len(summary_df) > 0:\n",
    "    print(\"\\nâœ… Recent Predictions:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Overall Statistics:\")\n",
    "    display(stats_df)\n",
    "    \n",
    "    # Create visualization of predictions\n",
    "    if len(summary_df) > 0:\n",
    "        fig = px.bar(\n",
    "            summary_df, \n",
    "            x='product_id', \n",
    "            y='avg_daily_demand',\n",
    "            color='store_id',\n",
    "            title='ğŸ“Š Average Daily Demand Predictions by Product',\n",
    "            height=500\n",
    "        )\n",
    "        fig.update_xaxis(tickangle=45)\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"âŒ No predictions found in database\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ INVENCARE ML ANALYTICS PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 13: Final Setup Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final setup instructions\n",
    "print(\"ğŸ¯ SETUP COMPLETE! Here's what you need to do next:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if 'endpoint_name' in locals():\n",
    "    print(\"\\nğŸ“‹ 1. UPDATE YOUR LAMBDA FUNCTION ENVIRONMENT VARIABLES:\")\n",
    "    print(f\"   LSTM_ENDPOINT_NAME={endpoint_name}\")\n",
    "    print(f\"   AWS_REGION={region}\")\n",
    "    print(f\"   DB_HOST={DB_CONFIG['host']}\")\n",
    "    print(f\"   DB_USER={DB_CONFIG['user']}\")\n",
    "    print(f\"   DB_PASSWORD={DB_CONFIG['password']}\")\n",
    "    print(f\"   DB_NAME={DB_CONFIG['database']}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ 1. DEPLOY THE MODEL FIRST:\")\n",
    "    print(\"   Re-run Step 8 to deploy your trained model\")\n",
    "\n",
    "print(\"\\nğŸ“‹ 2. UPDATE YOUR EXPRESS SERVER .ENV FILE:\")\n",
    "print(\"   LAMBDA_ML_ANALYTICS_FUNCTION=invencare-ml-analytics\")\n",
    "print(f\"   AWS_REGION={region}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ 3. DEPLOY YOUR LAMBDA FUNCTION:\")\n",
    "print(\"   Run: ./deploy-lambda-ml-analytics.sh\")\n",
    "print(\"   (Use the deployment script provided earlier)\")\n",
    "\n",
    "print(\"\\nğŸ“‹ 4. TEST YOUR SETUP:\")\n",
    "print(\"   â€¢ Go to your Forecasting page\")\n",
    "print(\"   â€¢ Click 'Refresh & Predict' button\")\n",
    "print(\"   â€¢ Check browser console for API calls\")\n",
    "print(\"   â€¢ Verify new predictions appear\")\n",
    "\n",
    "print(\"\\nğŸ“‹ 5. AVAILABLE API ENDPOINTS:\")\n",
    "print(\"   POST /api/ml/analytics - Main ML operations\")\n",
    "print(\"   GET  /api/ml/predictions/:product_id/:store_id - Get predictions\")\n",
    "print(\"   GET  /api/ml/dashboard - Dashboard data\")\n",
    "print(\"   GET  /api/ml/health - Health check\")\n",
    "\n",
    "print(\"\\nğŸ“Š 6. DATABASE TABLES CREATED:\")\n",
    "print(\"   â€¢ demand_predictions - Stores forecast results\")\n",
    "print(\"   â€¢ product_classifications - Stores ABC analysis (coming soon)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Your ML-powered inventory system is ready!\")\n",
    "print(\"ğŸ¯ Features enabled:\")\n",
    "print(\"   âœ… Real-time demand forecasting\")\n",
    "print(\"   âœ… Database-connected training\")\n",
    "print(\"   âœ… SageMaker deployment\")\n",
    "print(\"   âœ… Lambda integration\")\n",
    "print(\"   âœ… Express API endpoints\")\n",
    "print(\"   âœ… Frontend refresh automation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save important information to file\n",
    "if 'endpoint_name' in locals():\n",
    "    setup_info = {\n",
    "        'endpoint_name': endpoint_name,\n",
    "        'model_data': lstm_estimator.model_data if 'lstm_estimator' in locals() else None,\n",
    "        'training_job': lstm_estimator.latest_training_job.job_name if 'lstm_estimator' in locals() else None,\n",
    "        'region': region,\n",
    "        'bucket': bucket,\n",
    "        'database_tables': ['demand_predictions', 'product_classifications'],\n",
    "        'api_endpoints': [\n",
    "            'POST /api/ml/analytics',\n",
    "            'GET /api/ml/predictions/:product_id/:store_id',\n",
    "            'GET /api/ml/dashboard',\n",
    "            'GET /api/ml/health'\n",
    "        ],\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open('/tmp/invencare_ml_setup.json', 'w') as f:\n",
    "        json.dump(setup_info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Setup information saved to: /tmp/invencare_ml_setup.json\")\n",
    "\n",
    "print(\"\\nğŸ‰ Congratulations! Your InvenCare ML Analytics system is ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
