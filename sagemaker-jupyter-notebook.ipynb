{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Database-Connected ML Pipeline\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Connect to your MySQL database\n",
    "2. Train ML models using your transaction data\n",
    "3. Deploy models to SageMaker endpoints\n",
    "4. Make predictions and store results back in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pymysql pandas numpy scikit-learn tensorflow prophet statsmodels boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.sklearn import SKLearn\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SageMaker session setup\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"SageMaker bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'your-rds-endpoint.amazonaws.com',  # Replace with your RDS endpoint\n",
    "    'user': 'admin',\n",
    "    'password': 'your-password',  # Replace with your password\n",
    "    'database': 'inventory_management',\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create database connection\"\"\"\n",
    "    return pymysql.connect(**DB_CONFIG)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_db_connection()\n",
    "    print(\"‚úÖ Database connection successful\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data():\n",
    "    \"\"\"Fetch transaction data for ML training\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        product_name,\n",
    "        category,\n",
    "        store_id,\n",
    "        DATE(created_at) as date,\n",
    "        SUM(CASE WHEN transaction_type = 'Sale' THEN quantity ELSE 0 END) as sales_quantity,\n",
    "        SUM(CASE WHEN transaction_type = 'Sale' THEN total_amount ELSE 0 END) as sales_amount,\n",
    "        AVG(unit_price) as avg_price,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM inventory_transactions \n",
    "    WHERE created_at >= DATE_SUB(NOW(), INTERVAL 180 DAY)\n",
    "    GROUP BY product_id, store_id, DATE(created_at)\n",
    "    ORDER BY product_id, store_id, date\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load training data\n",
    "training_data = fetch_training_data()\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Sales trends by category\n",
    "plt.subplot(2, 2, 1)\n",
    "category_sales = training_data.groupby('category')['sales_amount'].sum().sort_values(ascending=False)\n",
    "category_sales.plot(kind='bar')\n",
    "plt.title('Sales by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Daily sales trend\n",
    "plt.subplot(2, 2, 2)\n",
    "daily_sales = training_data.groupby('date')['sales_amount'].sum()\n",
    "daily_sales.plot()\n",
    "plt.title('Daily Sales Trend')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Top products by sales\n",
    "plt.subplot(2, 2, 3)\n",
    "top_products = training_data.groupby('product_name')['sales_quantity'].sum().nlargest(10)\n",
    "top_products.plot(kind='barh')\n",
    "plt.title('Top 10 Products by Quantity Sold')\n",
    "\n",
    "# Store performance\n",
    "plt.subplot(2, 2, 4)\n",
    "store_sales = training_data.groupby('store_id')['sales_amount'].sum()\n",
    "store_sales.plot(kind='bar')\n",
    "plt.title('Sales by Store')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Different ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(product_id, store_id, sequence_length=30):\n",
    "    \"\"\"Prepare time series data for LSTM model\"\"\"\n",
    "    product_data = training_data[\n",
    "        (training_data['product_id'] == product_id) & \n",
    "        (training_data['store_id'] == store_id)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(product_data) < sequence_length + 10:\n",
    "        return None, None\n",
    "    \n",
    "    # Create complete date range and fill missing values\n",
    "    date_range = pd.date_range(\n",
    "        start=product_data['date'].min(),\n",
    "        end=product_data['date'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    complete_data = pd.DataFrame({'date': date_range})\n",
    "    complete_data = complete_data.merge(product_data, on='date', how='left')\n",
    "    complete_data['sales_quantity'] = complete_data['sales_quantity'].fillna(0)\n",
    "    \n",
    "    # Create sequences\n",
    "    sales_data = complete_data['sales_quantity'].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(sales_data)):\n",
    "        X.append(sales_data[i-sequence_length:i])\n",
    "        y.append(sales_data[i])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_classification_features():\n",
    "    \"\"\"Prepare features for ABC classification\"\"\"\n",
    "    features = training_data.groupby(['product_id', 'store_id']).agg({\n",
    "        'sales_quantity': ['sum', 'mean', 'std', 'count'],\n",
    "        'sales_amount': ['sum', 'mean', 'std'],\n",
    "        'avg_price': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    features.columns = ['_'.join(col).strip() for col in features.columns]\n",
    "    \n",
    "    # Add derived features\n",
    "    features['revenue_per_transaction'] = features['sales_amount_sum'] / features['sales_quantity_count']\n",
    "    features['price_volatility'] = features['avg_price_std'] / features['avg_price_mean']\n",
    "    features['demand_volatility'] = features['sales_quantity_std'] / features['sales_quantity_mean']\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    features = features.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Prepare sample data\n",
    "unique_products = training_data[['product_id', 'store_id']].drop_duplicates()\n",
    "print(f\"Unique product-store combinations: {len(unique_products)}\")\n",
    "\n",
    "# Show sample LSTM data preparation\n",
    "sample_product = unique_products.iloc[0]\n",
    "X_sample, y_sample = prepare_lstm_data(sample_product['product_id'], sample_product['store_id'])\n",
    "\n",
    "if X_sample is not None:\n",
    "    print(f\"Sample LSTM data - X shape: {X_sample.shape}, y shape: {y_sample.shape}\")\n",
    "else:\n",
    "    print(\"Insufficient data for sample product\")\n",
    "\n",
    "# Show classification features\n",
    "classification_features = prepare_classification_features()\n",
    "print(f\"\\nClassification features shape: {classification_features.shape}\")\n",
    "print(\"\\nFeature names:\")\n",
    "print(list(classification_features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LSTM Model with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data to S3\n",
    "s3_client = boto3.client('s3')\n",
    "training_data_key = 'training-data/transactions.csv'\n",
    "\n",
    "# Upload training data\n",
    "training_data.to_csv('/tmp/transactions.csv', index=False)\n",
    "s3_client.upload_file('/tmp/transactions.csv', bucket, training_data_key)\n",
    "\n",
    "print(f\"Training data uploaded to s3://{bucket}/{training_data_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM training script\n",
    "lstm_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_lstm_model(seq_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(seq_length, n_features)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    parser.add_argument('--seq-length', type=int, default=30)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(os.path.join(args.train, 'transactions.csv'))\n",
    "    \n",
    "    # Prepare aggregated daily sales data\n",
    "    daily_sales = df.groupby('date')['sales_quantity'].sum().sort_index()\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(daily_sales.values.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data.flatten(), args.seq_length)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Reshape for LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_lstm_model(args.seq_length)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(args.model_dir, 'lstm_model.h5'))\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, os.path.join(args.model_dir, 'scaler.pkl'))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'seq_length': args.seq_length,\n",
    "        'test_mae': float(test_mae),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'model_type': 'lstm_demand_forecasting'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(args.model_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    print(f\"Training completed. Test MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "# Save training script\n",
    "with open('lstm_training.py', 'w') as f:\n",
    "    f.write(lstm_script)\n",
    "\n",
    "print(\"LSTM training script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model with SageMaker\n",
    "lstm_estimator = TensorFlow(\n",
    "    entry_point='lstm_training.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    framework_version='2.8.0',\n",
    "    py_version='py39',\n",
    "    hyperparameters={\n",
    "        'epochs': 50,\n",
    "        'batch-size': 32,\n",
    "        'seq-length': 30\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start training\n",
    "training_input = f's3://{bucket}/training-data/'\n",
    "lstm_estimator.fit({'training': training_input})\n",
    "\n",
    "print(\"LSTM model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploy Model to SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference script\n",
    "inference_script = \"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = tf.keras.models.load_model(os.path.join(model_dir, 'lstm_model.h5'))\n",
    "    scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))\n",
    "    \n",
    "    with open(os.path.join(model_dir, 'metadata.json'), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    return {'model': model, 'scaler': scaler, 'metadata': metadata}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'application/json':\n",
    "        return json.loads(request_body)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    model = model_dict['model']\n",
    "    scaler = model_dict['scaler']\n",
    "    metadata = model_dict['metadata']\n",
    "    \n",
    "    historical_data = input_data.get('historical_data', [])\n",
    "    forecast_days = input_data.get('forecast_days', 30)\n",
    "    \n",
    "    if len(historical_data) == 0:\n",
    "        return {'error': 'No historical data provided'}\n",
    "    \n",
    "    # Scale input data\n",
    "    scaled_data = scaler.transform(np.array(historical_data).reshape(-1, 1))\n",
    "    \n",
    "    seq_length = metadata['seq_length']\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    current_sequence = scaled_data[-seq_length:].flatten()\n",
    "    \n",
    "    for _ in range(forecast_days):\n",
    "        model_input = current_sequence.reshape(1, seq_length, 1)\n",
    "        next_pred = model.predict(model_input)[0][0]\n",
    "        predictions.append(next_pred)\n",
    "        \n",
    "        # Update sequence\n",
    "        current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "    \n",
    "    # Inverse transform\n",
    "    actual_predictions = scaler.inverse_transform(\n",
    "        np.array(predictions).reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    actual_predictions = np.maximum(actual_predictions, 0)\n",
    "    \n",
    "    return {\n",
    "        'predictions': actual_predictions.tolist(),\n",
    "        'confidence_lower': (actual_predictions * 0.8).tolist(),\n",
    "        'confidence_upper': (actual_predictions * 1.2).tolist(),\n",
    "        'model_accuracy': metadata.get('test_mae', 0.0),\n",
    "        'forecast_horizon': forecast_days\n",
    "    }\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\"\"\"\n",
    "\n",
    "# Save inference script\n",
    "with open('inference.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"Inference script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "endpoint_name = f'demand-forecasting-{datetime.now().strftime(\"%Y%m%d%H%M\")}\n",
    "\n",
    "predictor = lstm_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Predictions and Store in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_data = {\n",
    "    'historical_data': [10, 12, 15, 8, 20, 18, 14, 16, 11, 13, 19, 22, 17, 9, 25, 21, 16, 12, 14, 18, 23, 19, 15, 11, 24, 20, 17, 13, 16, 22],\n",
    "    'forecast_days': 7\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "result = predictor.predict(test_data)\n",
    "print(\"Prediction result:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_in_db(product_id, store_id, prediction_result):\n",
    "    \"\"\"Store prediction results in database\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create predictions table if not exists\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS demand_predictions (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        product_id VARCHAR(50) NOT NULL,\n",
    "        store_id VARCHAR(50) NOT NULL,\n",
    "        model_type VARCHAR(50) NOT NULL,\n",
    "        forecast_days INT NOT NULL,\n",
    "        predictions JSON NOT NULL,\n",
    "        confidence_lower JSON,\n",
    "        confidence_upper JSON,\n",
    "        model_accuracy DECIMAL(5,4),\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        INDEX idx_product_store (product_id, store_id),\n",
    "        INDEX idx_created_at (created_at)\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    \n",
    "    # Insert prediction\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO demand_predictions \n",
    "    (product_id, store_id, model_type, forecast_days, predictions, confidence_lower, confidence_upper, model_accuracy)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    values = (\n",
    "        product_id,\n",
    "        store_id,\n",
    "        'lstm',\n",
    "        prediction_result['forecast_horizon'],\n",
    "        json.dumps(prediction_result['predictions']),\n",
    "        json.dumps(prediction_result['confidence_lower']),\n",
    "        json.dumps(prediction_result['confidence_upper']),\n",
    "        prediction_result['model_accuracy']\n",
    "    )\n",
    "    \n",
    "    cursor.execute(insert_query, values)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"‚úÖ Prediction stored for product {product_id} in store {store_id}\")\n",
    "\n",
    "# Store test prediction\n",
    "store_prediction_in_db('FV-BAN-001', 'store_001', result)\n",
    "\n",
    "# Verify stored prediction\n",
    "conn = get_db_connection()\n",
    "stored_predictions = pd.read_sql(\n",
    "    \"SELECT * FROM demand_predictions ORDER BY created_at DESC LIMIT 5\", \n",
    "    conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nRecent predictions in database:\")\n",
    "print(stored_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Predictions for All Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_predictions():\n",
    "    \"\"\"Run predictions for all products with sufficient data\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    \n",
    "    # Get products with sufficient transaction history\n",
    "    query = \"\"\"\n",
    "    SELECT product_id, store_id, COUNT(*) as transaction_count\n",
    "    FROM inventory_transactions \n",
    "    WHERE created_at >= DATE_SUB(NOW(), INTERVAL 90 DAY)\n",
    "    AND transaction_type = 'Sale'\n",
    "    GROUP BY product_id, store_id\n",
    "    HAVING COUNT(*) >= 10\n",
    "    ORDER BY transaction_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    products_to_predict = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(products_to_predict)} products for batch prediction\")\n",
    "    \n",
    "    # Run predictions for top products\n",
    "    successful_predictions = 0\n",
    "    \n",
    "    for idx, product in products_to_predict.head(5).iterrows():  # Limit to 5 for demo\n",
    "        try:\n",
    "            # Get historical data for this product\n",
    "            conn = get_db_connection()\n",
    "            history_query = \"\"\"\n",
    "            SELECT DATE(created_at) as date, SUM(quantity) as sales\n",
    "            FROM inventory_transactions\n",
    "            WHERE product_id = %s AND store_id = %s AND transaction_type = 'Sale'\n",
    "            AND created_at >= DATE_SUB(NOW(), INTERVAL 60 DAY)\n",
    "            GROUP BY DATE(created_at)\n",
    "            ORDER BY date\n",
    "            \"\"\"\n",
    "            \n",
    "            history_df = pd.read_sql(history_query, conn, params=[product['product_id'], product['store_id']])\n",
    "            conn.close()\n",
    "            \n",
    "            if len(history_df) >= 30:  # Need at least 30 days\n",
    "                historical_data = history_df['sales'].tolist()\n",
    "                \n",
    "                # Make prediction\n",
    "                test_data = {\n",
    "                    'historical_data': historical_data,\n",
    "                    'forecast_days': 30\n",
    "                }\n",
    "                \n",
    "                prediction_result = predictor.predict(test_data)\n",
    "                \n",
    "                # Store in database\n",
    "                store_prediction_in_db(\n",
    "                    product['product_id'], \n",
    "                    product['store_id'], \n",
    "                    prediction_result\n",
    "                )\n",
    "                \n",
    "                successful_predictions += 1\n",
    "                print(f\"‚úÖ Prediction {successful_predictions}: {product['product_id']} - {product['store_id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed prediction for {product['product_id']}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nBatch prediction completed: {successful_predictions} successful predictions\")\n",
    "\n",
    "# Run batch predictions\n",
    "run_batch_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_predictions():\n",
    "    conn = get_db_connection()\n",
    "    \n",
    "    # Get recent predictions\n",
    "    predictions_df = pd.read_sql(\n",
    "        \"SELECT * FROM demand_predictions ORDER BY created_at DESC LIMIT 3\",\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    \n",
    "    if len(predictions_df) == 0:\n",
    "        print(\"No predictions found in database\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(predictions_df), 1, figsize=(12, 4*len(predictions_df)))\n",
    "    \n",
    "    if len(predictions_df) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (_, prediction) in enumerate(predictions_df.iterrows()):\n",
    "        predictions = json.loads(prediction['predictions'])\n",
    "        confidence_lower = json.loads(prediction['confidence_lower'])\n",
    "        confidence_upper = json.loads(prediction['confidence_upper'])\n",
    "        \n",
    "        days = list(range(1, len(predictions) + 1))\n",
    "        \n",
    "        axes[idx].plot(days, predictions, 'b-', label='Prediction', linewidth=2)\n",
    "        axes[idx].fill_between(days, confidence_lower, confidence_upper, \n",
    "                              alpha=0.3, color='blue', label='Confidence Interval')\n",
    "        \n",
    "        axes[idx].set_title(f'Demand Forecast - {prediction[\"product_id\"]} ({prediction[\"store_id\"]})')\n",
    "        axes[idx].set_xlabel('Days')\n",
    "        axes[idx].set_ylabel('Predicted Demand')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions\n",
    "visualize_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Integration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API integration (assuming your Express server is running)\n",
    "import requests\n",
    "\n",
    "# Update these with your actual endpoint details\n",
    "API_BASE_URL = 'http://localhost:8080/api/ml'  # Replace with your server URL\n",
    "ENDPOINT_NAME = endpoint_name  # The endpoint we just created\n",
    "\n",
    "# Update environment variables in your Express server\n",
    "print(f\"Set these environment variables in your Express server:\")\n",
    "print(f\"LSTM_ENDPOINT_NAME={ENDPOINT_NAME}\")\n",
    "print(f\"AWS_REGION={region}\")\n",
    "\n",
    "# Test API call (uncomment when your server is running)\n",
    "# test_request = {\n",
    "#     'product_id': 'FV-BAN-001',\n",
    "#     'store_id': 'store_001',\n",
    "#     'forecast_days': 14,\n",
    "#     'model_type': 'lstm'\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     response = requests.post(f'{API_BASE_URL}/forecast', json=test_request)\n",
    "#     print(\"API Response:\")\n",
    "#     print(json.dumps(response.json(), indent=2))\n",
    "# except Exception as e:\n",
    "#     print(f\"API test failed: {e}\")\n",
    "#     print(\"Make sure your Express server is running with the ML routes enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've accomplished\n",
    "print(\"üéâ SageMaker Database Integration Complete!\")\n",
    "print(\"\\nWhat we've accomplished:\")\n",
    "print(\"‚úÖ Connected to MySQL database\")\n",
    "print(\"‚úÖ Extracted and analyzed transaction data\")\n",
    "print(\"‚úÖ Trained LSTM model with SageMaker\")\n",
    "print(\"‚úÖ Deployed model to real-time endpoint\")\n",
    "print(\"‚úÖ Made predictions and stored results in database\")\n",
    "print(\"‚úÖ Created API integration layer\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Update your Express server environment variables\")\n",
    "print(\"2. Test the ML API endpoints\")\n",
    "print(\"3. Integrate predictions into your frontend\")\n",
    "print(\"4. Set up automated retraining schedule\")\n",
    "print(\"5. Add more ML models (ARIMA, Prophet, Classification)\")\n",
    "\n",
    "print(f\"\\nEndpoint created: {endpoint_name}\")\n",
    "print(f\"S3 bucket used: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up endpoint to save costs\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted to save costs\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
